nohup: 忽略输入
/home/dd/miniconda3/envs/pyspompts/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
./logs/reproduce_1993_lora_prompt_lora_prompt_clip_domainnet_345_345_2024-06-13-10:06:16
2024-06-13 10:06:16,192 [trainer.py] => config: configs/lora-prompt/domainnet.json
2024-06-13 10:06:16,192 [trainer.py] => prefix: reproduce
2024-06-13 10:06:16,192 [trainer.py] => dataset: domainnet
2024-06-13 10:06:16,192 [trainer.py] => data_path: /home/dd/s-prompts/data/domainnet/
2024-06-13 10:06:16,192 [trainer.py] => task_name: ['clipart', 'infograph', 'painting', 'quickdraw', 'real', 'sketch']
2024-06-13 10:06:16,192 [trainer.py] => memory_size: 0
2024-06-13 10:06:16,192 [trainer.py] => memory_per_class: 0
2024-06-13 10:06:16,193 [trainer.py] => fixed_memory: True
2024-06-13 10:06:16,193 [trainer.py] => shuffle: False
2024-06-13 10:06:16,193 [trainer.py] => init_cls: 345
2024-06-13 10:06:16,193 [trainer.py] => increment: 345
2024-06-13 10:06:16,193 [trainer.py] => model_name: lora_prompt
2024-06-13 10:06:16,193 [trainer.py] => net_type: lora_prompt_clip
2024-06-13 10:06:16,193 [trainer.py] => embd_dim: 768
2024-06-13 10:06:16,193 [trainer.py] => prompt_length: 10
2024-06-13 10:06:16,193 [trainer.py] => total_sessions: 6
2024-06-13 10:06:16,193 [trainer.py] => device: [device(type='cuda', index=1)]
2024-06-13 10:06:16,193 [trainer.py] => seed: 1993
2024-06-13 10:06:16,193 [trainer.py] => EPSILON: 1e-08
2024-06-13 10:06:16,193 [trainer.py] => init_epoch: 30
2024-06-13 10:06:16,193 [trainer.py] => init_lr: 0.01
2024-06-13 10:06:16,193 [trainer.py] => init_lr_decay: 0.1
2024-06-13 10:06:16,193 [trainer.py] => init_weight_decay: 0.0005
2024-06-13 10:06:16,193 [trainer.py] => epochs: 30
2024-06-13 10:06:16,193 [trainer.py] => lrate: 0.01
2024-06-13 10:06:16,193 [trainer.py] => lrate_decay: 0.1
2024-06-13 10:06:16,193 [trainer.py] => batch_size: 128
2024-06-13 10:06:16,193 [trainer.py] => weight_decay: 0.0002
2024-06-13 10:06:16,193 [trainer.py] => num_workers: 16
2024-06-13 10:06:17,587 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069]
/home/dd/miniconda3/envs/pyspompts/lib/python3.8/site-packages/peft/tuners/lora.py:173: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.
  warnings.warn(
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
2024-06-13 10:07:47,782 [trainer.py] => All params: 668051457
2024-06-13 10:07:47,787 [trainer.py] => Trainable params: 150895617
2024-06-13 10:07:47,787 [lora_prompt.py] => Learning on 0-345
Parameters to be updated: {'lora_image_model_pool.0.base_model.model.transformer.resblocks.9.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.3.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.1.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.10.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.11.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.4.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.8.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.10.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.6.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.11.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.7.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.0.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.3.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.4.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.7.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.0.attn.out_proj.lora_A.weight', 'classifier_pool.0.ctx', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.5.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.2.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.2.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.5.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.8.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.1.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.6.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.9.attn.out_proj.lora_A.weight'},count:204800
  0%|          | 0/30 [00:00<?, ?it/s]Task 0, Epoch 1/30 => Loss 1.853, Train_accy 60.31, Test_accy 74.47:   0%|          | 0/30 [02:11<?, ?it/s]Task 0, Epoch 1/30 => Loss 1.853, Train_accy 60.31, Test_accy 74.47:   3%|▎         | 1/30 [02:11<1:03:31, 131.42s/it]Task 0, Epoch 2/30 => Loss 1.739, Train_accy 62.08, Test_accy 75.43:   3%|▎         | 1/30 [04:26<1:03:31, 131.42s/it]Task 0, Epoch 2/30 => Loss 1.739, Train_accy 62.08, Test_accy 75.43:   7%|▋         | 2/30 [04:26<1:02:21, 133.61s/it]Task 0, Epoch 3/30 => Loss 1.696, Train_accy 62.83, Test_accy 75.97:   7%|▋         | 2/30 [06:43<1:02:21, 133.61s/it]Task 0, Epoch 3/30 => Loss 1.696, Train_accy 62.83, Test_accy 75.97:  10%|█         | 3/30 [06:43<1:00:51, 135.23s/it]Task 0, Epoch 4/30 => Loss 1.673, Train_accy 63.22, Test_accy 76.41:  10%|█         | 3/30 [08:59<1:00:51, 135.23s/it]Task 0, Epoch 4/30 => Loss 1.673, Train_accy 63.22, Test_accy 76.41:  13%|█▎        | 4/30 [08:59<58:46, 135.62s/it]  Task 0, Epoch 5/30 => Loss 1.631, Train_accy 63.80, Test_accy 76.47:  13%|█▎        | 4/30 [11:19<58:46, 135.62s/it]Task 0, Epoch 5/30 => Loss 1.631, Train_accy 63.80, Test_accy 76.47:  17%|█▋        | 5/30 [11:19<57:09, 137.18s/it]Task 0, Epoch 6/30 => Loss 1.626, Train_accy 64.20, Test_accy 76.83:  17%|█▋        | 5/30 [13:37<57:09, 137.18s/it]Task 0, Epoch 6/30 => Loss 1.626, Train_accy 64.20, Test_accy 76.83:  20%|██        | 6/30 [13:37<54:52, 137.20s/it]Task 0, Epoch 7/30 => Loss 1.634, Train_accy 63.93, Test_accy 77.16:  20%|██        | 6/30 [15:53<54:52, 137.20s/it]Task 0, Epoch 7/30 => Loss 1.634, Train_accy 63.93, Test_accy 77.16:  23%|██▎       | 7/30 [15:53<52:26, 136.81s/it]Task 0, Epoch 8/30 => Loss 1.613, Train_accy 64.22, Test_accy 77.03:  23%|██▎       | 7/30 [18:09<52:26, 136.81s/it]Task 0, Epoch 8/30 => Loss 1.613, Train_accy 64.22, Test_accy 77.03:  27%|██▋       | 8/30 [18:09<50:03, 136.53s/it]Task 0, Epoch 9/30 => Loss 1.589, Train_accy 64.74, Test_accy 77.23:  27%|██▋       | 8/30 [20:25<50:03, 136.53s/it]Task 0, Epoch 9/30 => Loss 1.589, Train_accy 64.74, Test_accy 77.23:  30%|███       | 9/30 [20:25<47:44, 136.39s/it]Task 0, Epoch 10/30 => Loss 1.591, Train_accy 64.48, Test_accy 77.10:  30%|███       | 9/30 [22:41<47:44, 136.39s/it]Task 0, Epoch 10/30 => Loss 1.591, Train_accy 64.48, Test_accy 77.10:  33%|███▎      | 10/30 [22:41<45:29, 136.45s/it]Task 0, Epoch 11/30 => Loss 1.580, Train_accy 64.88, Test_accy 77.31:  33%|███▎      | 10/30 [24:58<45:29, 136.45s/it]Task 0, Epoch 11/30 => Loss 1.580, Train_accy 64.88, Test_accy 77.31:  37%|███▋      | 11/30 [24:58<43:14, 136.55s/it]Task 0, Epoch 12/30 => Loss 1.558, Train_accy 65.20, Test_accy 77.40:  37%|███▋      | 11/30 [27:14<43:14, 136.55s/it]Task 0, Epoch 12/30 => Loss 1.558, Train_accy 65.20, Test_accy 77.40:  40%|████      | 12/30 [27:14<40:52, 136.26s/it]Task 0, Epoch 13/30 => Loss 1.562, Train_accy 65.11, Test_accy 77.76:  40%|████      | 12/30 [29:30<40:52, 136.26s/it]Task 0, Epoch 13/30 => Loss 1.562, Train_accy 65.11, Test_accy 77.76:  43%|████▎     | 13/30 [29:30<38:35, 136.20s/it]Task 0, Epoch 14/30 => Loss 1.553, Train_accy 65.54, Test_accy 77.59:  43%|████▎     | 13/30 [31:45<38:35, 136.20s/it]Task 0, Epoch 14/30 => Loss 1.553, Train_accy 65.54, Test_accy 77.59:  47%|████▋     | 14/30 [31:45<36:15, 136.00s/it]Task 0, Epoch 15/30 => Loss 1.534, Train_accy 65.74, Test_accy 77.55:  47%|████▋     | 14/30 [34:02<36:15, 136.00s/it]Task 0, Epoch 15/30 => Loss 1.534, Train_accy 65.74, Test_accy 77.55:  50%|█████     | 15/30 [34:02<34:01, 136.13s/it]Task 0, Epoch 16/30 => Loss 1.543, Train_accy 65.60, Test_accy 77.88:  50%|█████     | 15/30 [36:18<34:01, 136.13s/it]Task 0, Epoch 16/30 => Loss 1.543, Train_accy 65.60, Test_accy 77.88:  53%|█████▎    | 16/30 [36:18<31:46, 136.20s/it]Task 0, Epoch 17/30 => Loss 1.538, Train_accy 65.58, Test_accy 77.54:  53%|█████▎    | 16/30 [38:33<31:46, 136.20s/it]Task 0, Epoch 17/30 => Loss 1.538, Train_accy 65.58, Test_accy 77.54:  57%|█████▋    | 17/30 [38:33<29:27, 135.95s/it]Task 0, Epoch 18/30 => Loss 1.536, Train_accy 65.75, Test_accy 77.86:  57%|█████▋    | 17/30 [40:49<29:27, 135.95s/it]Task 0, Epoch 18/30 => Loss 1.536, Train_accy 65.75, Test_accy 77.86:  60%|██████    | 18/30 [40:49<27:08, 135.72s/it]Task 0, Epoch 19/30 => Loss 1.513, Train_accy 66.28, Test_accy 77.62:  60%|██████    | 18/30 [43:05<27:08, 135.72s/it]Task 0, Epoch 19/30 => Loss 1.513, Train_accy 66.28, Test_accy 77.62:  63%|██████▎   | 19/30 [43:05<24:55, 135.92s/it]Task 0, Epoch 20/30 => Loss 1.535, Train_accy 65.89, Test_accy 77.81:  63%|██████▎   | 19/30 [45:19<24:55, 135.92s/it]Task 0, Epoch 20/30 => Loss 1.535, Train_accy 65.89, Test_accy 77.81:  67%|██████▋   | 20/30 [45:19<22:34, 135.47s/it]Task 0, Epoch 21/30 => Loss 1.514, Train_accy 66.25, Test_accy 77.99:  67%|██████▋   | 20/30 [47:34<22:34, 135.47s/it]Task 0, Epoch 21/30 => Loss 1.514, Train_accy 66.25, Test_accy 77.99:  70%|███████   | 21/30 [47:34<20:17, 135.29s/it]Task 0, Epoch 22/30 => Loss 1.507, Train_accy 66.32, Test_accy 77.85:  70%|███████   | 21/30 [49:50<20:17, 135.29s/it]Task 0, Epoch 22/30 => Loss 1.507, Train_accy 66.32, Test_accy 77.85:  73%|███████▎  | 22/30 [49:50<18:03, 135.42s/it]Task 0, Epoch 23/30 => Loss 1.510, Train_accy 66.19, Test_accy 78.05:  73%|███████▎  | 22/30 [52:04<18:03, 135.42s/it]Task 0, Epoch 23/30 => Loss 1.510, Train_accy 66.19, Test_accy 78.05:  77%|███████▋  | 23/30 [52:04<15:44, 134.97s/it]Task 0, Epoch 24/30 => Loss 1.492, Train_accy 66.43, Test_accy 77.88:  77%|███████▋  | 23/30 [54:18<15:44, 134.97s/it]Task 0, Epoch 24/30 => Loss 1.492, Train_accy 66.43, Test_accy 77.88:  80%|████████  | 24/30 [54:18<13:27, 134.58s/it]Task 0, Epoch 25/30 => Loss 1.498, Train_accy 66.28, Test_accy 78.14:  80%|████████  | 24/30 [56:30<13:27, 134.58s/it]Task 0, Epoch 25/30 => Loss 1.498, Train_accy 66.28, Test_accy 78.14:  83%|████████▎ | 25/30 [56:30<11:10, 134.08s/it]Task 0, Epoch 26/30 => Loss 1.490, Train_accy 66.25, Test_accy 78.14:  83%|████████▎ | 25/30 [58:49<11:10, 134.08s/it]Task 0, Epoch 26/30 => Loss 1.490, Train_accy 66.25, Test_accy 78.14:  87%|████████▋ | 26/30 [58:49<09:01, 135.43s/it]Task 0, Epoch 27/30 => Loss 1.475, Train_accy 66.72, Test_accy 78.08:  87%|████████▋ | 26/30 [1:01:07<09:01, 135.43s/it]Task 0, Epoch 27/30 => Loss 1.475, Train_accy 66.72, Test_accy 78.08:  90%|█████████ | 27/30 [1:01:07<06:48, 136.15s/it]Task 0, Epoch 28/30 => Loss 1.488, Train_accy 66.60, Test_accy 78.09:  90%|█████████ | 27/30 [1:03:23<06:48, 136.15s/it]Task 0, Epoch 28/30 => Loss 1.488, Train_accy 66.60, Test_accy 78.09:  93%|█████████▎| 28/30 [1:03:23<04:32, 136.21s/it]Task 0, Epoch 29/30 => Loss 1.497, Train_accy 66.38, Test_accy 78.12:  93%|█████████▎| 28/30 [1:05:40<04:32, 136.21s/it]Task 0, Epoch 29/30 => Loss 1.497, Train_accy 66.38, Test_accy 78.12:  97%|█████████▋| 29/30 [1:05:40<02:16, 136.45s/it]Task 0, Epoch 30/30 => Loss 1.485, Train_accy 66.68, Test_accy 78.12:  97%|█████████▋| 29/30 [1:07:58<02:16, 136.45s/it]Task 0, Epoch 30/30 => Loss 1.485, Train_accy 66.68, Test_accy 78.12: 100%|██████████| 30/30 [1:07:58<00:00, 136.83s/it]Task 0, Epoch 30/30 => Loss 1.485, Train_accy 66.68, Test_accy 78.12: 100%|██████████| 30/30 [1:07:58<00:00, 135.95s/it]
2024-06-13 11:15:48,998 [lora_prompt.py] => Task 0, Epoch 30/30 => Loss 1.485, Train_accy 66.68, Test_accy 78.12
2024-06-13 11:19:48,994 [lora_prompt.py] => Exemplar size: 0
2024-06-13 11:19:48,995 [trainer.py] => CNN: {'total': 78.1, '00-344': 78.1, 'old': 0, 'new': 78.1}
2024-06-13 11:19:48,995 [trainer.py] => CNN top1 curve: [78.1]
2024-06-13 11:19:49,002 [trainer.py] => All params: 668051457
2024-06-13 11:19:49,008 [trainer.py] => Trainable params: 204800
2024-06-13 11:19:49,008 [lora_prompt.py] => Learning on 345-690
Parameters to be updated: {'lora_image_model_pool.1.base_model.model.transformer.resblocks.7.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.0.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.2.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.3.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.9.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.9.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.2.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.8.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.10.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.1.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.6.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.4.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.8.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.6.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.4.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.5.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.10.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.5.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.11.attn.out_proj.lora_B.weight', 'classifier_pool.1.ctx', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.0.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.11.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.3.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.7.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.1.attn.out_proj.lora_A.weight'},count:204800
  0%|          | 0/30 [00:00<?, ?it/s]Task 1, Epoch 1/30 => Loss 2.779, Train_accy 44.85, Test_accy 62.47:   0%|          | 0/30 [02:57<?, ?it/s]Task 1, Epoch 1/30 => Loss 2.779, Train_accy 44.85, Test_accy 62.47:   3%|▎         | 1/30 [02:57<1:25:53, 177.71s/it]Task 1, Epoch 2/30 => Loss 2.620, Train_accy 46.66, Test_accy 62.53:   3%|▎         | 1/30 [05:55<1:25:53, 177.71s/it]Task 1, Epoch 2/30 => Loss 2.620, Train_accy 46.66, Test_accy 62.53:   7%|▋         | 2/30 [05:55<1:22:53, 177.62s/it]Task 1, Epoch 3/30 => Loss 2.561, Train_accy 47.37, Test_accy 62.64:   7%|▋         | 2/30 [08:53<1:22:53, 177.62s/it]Task 1, Epoch 3/30 => Loss 2.561, Train_accy 47.37, Test_accy 62.64:  10%|█         | 3/30 [08:53<1:20:09, 178.12s/it]Task 1, Epoch 4/30 => Loss 2.553, Train_accy 46.99, Test_accy 62.09:  10%|█         | 3/30 [11:52<1:20:09, 178.12s/it]Task 1, Epoch 4/30 => Loss 2.553, Train_accy 46.99, Test_accy 62.09:  13%|█▎        | 4/30 [11:52<1:17:10, 178.11s/it]Task 1, Epoch 5/30 => Loss 2.528, Train_accy 47.62, Test_accy 62.44:  13%|█▎        | 4/30 [14:53<1:17:10, 178.11s/it]Task 1, Epoch 5/30 => Loss 2.528, Train_accy 47.62, Test_accy 62.44:  17%|█▋        | 5/30 [14:53<1:14:39, 179.19s/it]Task 1, Epoch 6/30 => Loss 2.524, Train_accy 47.66, Test_accy 62.20:  17%|█▋        | 5/30 [17:53<1:14:39, 179.19s/it]Task 1, Epoch 6/30 => Loss 2.524, Train_accy 47.66, Test_accy 62.20:  20%|██        | 6/30 [17:53<1:11:47, 179.47s/it]Task 1, Epoch 7/30 => Loss 2.498, Train_accy 47.68, Test_accy 62.18:  20%|██        | 6/30 [20:52<1:11:47, 179.47s/it]Task 1, Epoch 7/30 => Loss 2.498, Train_accy 47.68, Test_accy 62.18:  23%|██▎       | 7/30 [20:52<1:08:46, 179.40s/it]Task 1, Epoch 8/30 => Loss 2.484, Train_accy 48.14, Test_accy 62.41:  23%|██▎       | 7/30 [23:52<1:08:46, 179.40s/it]Task 1, Epoch 8/30 => Loss 2.484, Train_accy 48.14, Test_accy 62.41:  27%|██▋       | 8/30 [23:52<1:05:48, 179.46s/it]Task 1, Epoch 9/30 => Loss 2.467, Train_accy 48.54, Test_accy 62.34:  27%|██▋       | 8/30 [26:50<1:05:48, 179.46s/it]Task 1, Epoch 9/30 => Loss 2.467, Train_accy 48.54, Test_accy 62.34:  30%|███       | 9/30 [26:50<1:02:44, 179.25s/it]Task 1, Epoch 10/30 => Loss 2.474, Train_accy 47.94, Test_accy 62.23:  30%|███       | 9/30 [29:50<1:02:44, 179.25s/it]Task 1, Epoch 10/30 => Loss 2.474, Train_accy 47.94, Test_accy 62.23:  33%|███▎      | 10/30 [29:50<59:50, 179.50s/it] Task 1, Epoch 11/30 => Loss 2.456, Train_accy 48.34, Test_accy 62.69:  33%|███▎      | 10/30 [32:50<59:50, 179.50s/it]Task 1, Epoch 11/30 => Loss 2.456, Train_accy 48.34, Test_accy 62.69:  37%|███▋      | 11/30 [32:50<56:52, 179.62s/it]Task 1, Epoch 12/30 => Loss 2.437, Train_accy 48.87, Test_accy 61.67:  37%|███▋      | 11/30 [35:50<56:52, 179.62s/it]Task 1, Epoch 12/30 => Loss 2.437, Train_accy 48.87, Test_accy 61.67:  40%|████      | 12/30 [35:50<53:55, 179.73s/it]Task 1, Epoch 13/30 => Loss 2.448, Train_accy 48.74, Test_accy 61.83:  40%|████      | 12/30 [38:53<53:55, 179.73s/it]Task 1, Epoch 13/30 => Loss 2.448, Train_accy 48.74, Test_accy 61.83:  43%|████▎     | 13/30 [38:53<51:12, 180.74s/it]Task 1, Epoch 14/30 => Loss 2.447, Train_accy 48.48, Test_accy 61.62:  43%|████▎     | 13/30 [41:52<51:12, 180.74s/it]Task 1, Epoch 14/30 => Loss 2.447, Train_accy 48.48, Test_accy 61.62:  47%|████▋     | 14/30 [41:52<48:00, 180.04s/it]Task 1, Epoch 15/30 => Loss 2.434, Train_accy 48.54, Test_accy 62.49:  47%|████▋     | 14/30 [44:52<48:00, 180.04s/it]Task 1, Epoch 15/30 => Loss 2.434, Train_accy 48.54, Test_accy 62.49:  50%|█████     | 15/30 [44:52<44:59, 179.98s/it]Task 1, Epoch 16/30 => Loss 2.420, Train_accy 48.89, Test_accy 62.33:  50%|█████     | 15/30 [47:51<44:59, 179.98s/it]Task 1, Epoch 16/30 => Loss 2.420, Train_accy 48.89, Test_accy 62.33:  53%|█████▎    | 16/30 [47:51<41:58, 179.91s/it]Task 1, Epoch 17/30 => Loss 2.422, Train_accy 49.02, Test_accy 62.28:  53%|█████▎    | 16/30 [50:52<41:58, 179.91s/it]Task 1, Epoch 17/30 => Loss 2.422, Train_accy 49.02, Test_accy 62.28:  57%|█████▋    | 17/30 [50:52<39:03, 180.27s/it]Task 1, Epoch 18/30 => Loss 2.416, Train_accy 49.11, Test_accy 61.98:  57%|█████▋    | 17/30 [53:53<39:03, 180.27s/it]Task 1, Epoch 18/30 => Loss 2.416, Train_accy 49.11, Test_accy 61.98:  60%|██████    | 18/30 [53:53<36:05, 180.47s/it]Task 1, Epoch 19/30 => Loss 2.412, Train_accy 48.88, Test_accy 62.08:  60%|██████    | 18/30 [56:51<36:05, 180.47s/it]Task 1, Epoch 19/30 => Loss 2.412, Train_accy 48.88, Test_accy 62.08:  63%|██████▎   | 19/30 [56:51<32:57, 179.76s/it]Task 1, Epoch 20/30 => Loss 2.403, Train_accy 49.29, Test_accy 62.00:  63%|██████▎   | 19/30 [59:52<32:57, 179.76s/it]Task 1, Epoch 20/30 => Loss 2.403, Train_accy 49.29, Test_accy 62.00:  67%|██████▋   | 20/30 [59:52<30:00, 180.08s/it]Task 1, Epoch 21/30 => Loss 2.420, Train_accy 48.83, Test_accy 61.77:  67%|██████▋   | 20/30 [1:02:52<30:00, 180.08s/it]Task 1, Epoch 21/30 => Loss 2.420, Train_accy 48.83, Test_accy 61.77:  70%|███████   | 21/30 [1:02:52<26:59, 179.91s/it]Task 1, Epoch 22/30 => Loss 2.393, Train_accy 49.03, Test_accy 61.73:  70%|███████   | 21/30 [1:05:52<26:59, 179.91s/it]Task 1, Epoch 22/30 => Loss 2.393, Train_accy 49.03, Test_accy 61.73:  73%|███████▎  | 22/30 [1:05:52<23:59, 179.92s/it]Task 1, Epoch 23/30 => Loss 2.404, Train_accy 49.13, Test_accy 61.69:  73%|███████▎  | 22/30 [1:08:52<23:59, 179.92s/it]Task 1, Epoch 23/30 => Loss 2.404, Train_accy 49.13, Test_accy 61.69:  77%|███████▋  | 23/30 [1:08:52<20:59, 179.96s/it]Task 1, Epoch 24/30 => Loss 2.402, Train_accy 49.15, Test_accy 62.06:  77%|███████▋  | 23/30 [1:11:53<20:59, 179.96s/it]Task 1, Epoch 24/30 => Loss 2.402, Train_accy 49.15, Test_accy 62.06:  80%|████████  | 24/30 [1:11:53<18:01, 180.18s/it]Task 1, Epoch 25/30 => Loss 2.394, Train_accy 49.34, Test_accy 61.90:  80%|████████  | 24/30 [1:14:52<18:01, 180.18s/it]Task 1, Epoch 25/30 => Loss 2.394, Train_accy 49.34, Test_accy 61.90:  83%|████████▎ | 25/30 [1:14:52<14:59, 180.00s/it]Task 1, Epoch 26/30 => Loss 2.381, Train_accy 49.50, Test_accy 61.92:  83%|████████▎ | 25/30 [1:17:52<14:59, 180.00s/it]Task 1, Epoch 26/30 => Loss 2.381, Train_accy 49.50, Test_accy 61.92:  87%|████████▋ | 26/30 [1:17:52<11:59, 180.00s/it]Task 1, Epoch 27/30 => Loss 2.376, Train_accy 49.36, Test_accy 61.83:  87%|████████▋ | 26/30 [1:20:52<11:59, 180.00s/it]Task 1, Epoch 27/30 => Loss 2.376, Train_accy 49.36, Test_accy 61.83:  90%|█████████ | 27/30 [1:20:52<08:59, 179.91s/it]Task 1, Epoch 28/30 => Loss 2.404, Train_accy 49.14, Test_accy 61.92:  90%|█████████ | 27/30 [1:23:51<08:59, 179.91s/it]Task 1, Epoch 28/30 => Loss 2.404, Train_accy 49.14, Test_accy 61.92:  93%|█████████▎| 28/30 [1:23:51<05:59, 179.72s/it]Task 1, Epoch 29/30 => Loss 2.398, Train_accy 49.25, Test_accy 61.84:  93%|█████████▎| 28/30 [1:26:52<05:59, 179.72s/it]Task 1, Epoch 29/30 => Loss 2.398, Train_accy 49.25, Test_accy 61.84:  97%|█████████▋| 29/30 [1:26:52<02:59, 179.96s/it]Task 1, Epoch 30/30 => Loss 2.386, Train_accy 49.18, Test_accy 61.84:  97%|█████████▋| 29/30 [1:29:52<02:59, 179.96s/it]Task 1, Epoch 30/30 => Loss 2.386, Train_accy 49.18, Test_accy 61.84: 100%|██████████| 30/30 [1:29:52<00:00, 180.04s/it]Task 1, Epoch 30/30 => Loss 2.386, Train_accy 49.18, Test_accy 61.84: 100%|██████████| 30/30 [1:29:52<00:00, 179.74s/it]
2024-06-13 12:49:41,811 [lora_prompt.py] => Task 1, Epoch 30/30 => Loss 2.386, Train_accy 49.18, Test_accy 61.84
2024-06-13 12:57:13,648 [lora_prompt.py] => Exemplar size: 0
2024-06-13 12:57:13,648 [trainer.py] => CNN: {'total': 67.47, '00-344': 77.48, '345-689': 58.09, 'old': 77.48, 'new': 58.09}
2024-06-13 12:57:13,648 [trainer.py] => CNN top1 curve: [78.1, 67.47]
2024-06-13 12:57:13,655 [trainer.py] => All params: 668051457
2024-06-13 12:57:13,662 [trainer.py] => Trainable params: 204800
2024-06-13 12:57:13,662 [lora_prompt.py] => Learning on 690-1035
Parameters to be updated: {'lora_image_model_pool.2.base_model.model.transformer.resblocks.0.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.10.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.5.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.6.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.0.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.8.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.9.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.2.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.11.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.9.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.10.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.5.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.7.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.7.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.1.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.3.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.4.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.6.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.4.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.1.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.3.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.2.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.11.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.8.attn.out_proj.lora_B.weight', 'classifier_pool.2.ctx'},count:204800
  0%|          | 0/30 [00:00<?, ?it/s]Task 2, Epoch 1/30 => Loss 1.779, Train_accy 61.88, Test_accy 65.34:   0%|          | 0/30 [04:26<?, ?it/s]Task 2, Epoch 1/30 => Loss 1.779, Train_accy 61.88, Test_accy 65.34:   3%|▎         | 1/30 [04:26<2:08:48, 266.49s/it]Task 2, Epoch 2/30 => Loss 1.656, Train_accy 63.79, Test_accy 65.00:   3%|▎         | 1/30 [08:55<2:08:48, 266.49s/it]Task 2, Epoch 2/30 => Loss 1.656, Train_accy 63.79, Test_accy 65.00:   7%|▋         | 2/30 [08:55<2:05:06, 268.10s/it]Task 2, Epoch 3/30 => Loss 1.612, Train_accy 64.45, Test_accy 65.39:   7%|▋         | 2/30 [13:24<2:05:06, 268.10s/it]Task 2, Epoch 3/30 => Loss 1.612, Train_accy 64.45, Test_accy 65.39:  10%|█         | 3/30 [13:24<2:00:48, 268.47s/it]Task 2, Epoch 4/30 => Loss 1.572, Train_accy 65.28, Test_accy 64.93:  10%|█         | 3/30 [17:55<2:00:48, 268.47s/it]Task 2, Epoch 4/30 => Loss 1.572, Train_accy 65.28, Test_accy 64.93:  13%|█▎        | 4/30 [17:55<1:56:44, 269.41s/it]Task 2, Epoch 5/30 => Loss 1.568, Train_accy 65.22, Test_accy 64.75:  13%|█▎        | 4/30 [22:24<1:56:44, 269.41s/it]Task 2, Epoch 5/30 => Loss 1.568, Train_accy 65.22, Test_accy 64.75:  17%|█▋        | 5/30 [22:24<1:52:13, 269.36s/it]Task 2, Epoch 6/30 => Loss 1.559, Train_accy 65.38, Test_accy 64.75:  17%|█▋        | 5/30 [26:54<1:52:13, 269.36s/it]Task 2, Epoch 6/30 => Loss 1.559, Train_accy 65.38, Test_accy 64.75:  20%|██        | 6/30 [26:54<1:47:51, 269.64s/it]Task 2, Epoch 7/30 => Loss 1.539, Train_accy 65.61, Test_accy 64.47:  20%|██        | 6/30 [31:20<1:47:51, 269.64s/it]Task 2, Epoch 7/30 => Loss 1.539, Train_accy 65.61, Test_accy 64.47:  23%|██▎       | 7/30 [31:20<1:42:52, 268.37s/it]Task 2, Epoch 8/30 => Loss 1.532, Train_accy 65.79, Test_accy 64.49:  23%|██▎       | 7/30 [35:49<1:42:52, 268.37s/it]Task 2, Epoch 8/30 => Loss 1.532, Train_accy 65.79, Test_accy 64.49:  27%|██▋       | 8/30 [35:49<1:38:28, 268.59s/it]Task 2, Epoch 9/30 => Loss 1.521, Train_accy 65.94, Test_accy 64.07:  27%|██▋       | 8/30 [40:17<1:38:28, 268.59s/it]Task 2, Epoch 9/30 => Loss 1.521, Train_accy 65.94, Test_accy 64.07:  30%|███       | 9/30 [40:17<1:33:54, 268.30s/it]Task 2, Epoch 10/30 => Loss 1.501, Train_accy 66.33, Test_accy 64.69:  30%|███       | 9/30 [44:35<1:33:54, 268.30s/it]Task 2, Epoch 10/30 => Loss 1.501, Train_accy 66.33, Test_accy 64.69:  33%|███▎      | 10/30 [44:35<1:28:24, 265.24s/it]Task 2, Epoch 11/30 => Loss 1.515, Train_accy 66.06, Test_accy 64.47:  33%|███▎      | 10/30 [48:46<1:28:24, 265.24s/it]Task 2, Epoch 11/30 => Loss 1.515, Train_accy 66.06, Test_accy 64.47:  37%|███▋      | 11/30 [48:46<1:22:37, 260.91s/it]Task 2, Epoch 12/30 => Loss 1.504, Train_accy 66.30, Test_accy 64.04:  37%|███▋      | 11/30 [52:55<1:22:37, 260.91s/it]Task 2, Epoch 12/30 => Loss 1.504, Train_accy 66.30, Test_accy 64.04:  40%|████      | 12/30 [52:55<1:17:07, 257.08s/it]Task 2, Epoch 13/30 => Loss 1.488, Train_accy 66.64, Test_accy 64.23:  40%|████      | 12/30 [57:04<1:17:07, 257.08s/it]Task 2, Epoch 13/30 => Loss 1.488, Train_accy 66.64, Test_accy 64.23:  43%|████▎     | 13/30 [57:04<1:12:08, 254.63s/it]Task 2, Epoch 14/30 => Loss 1.502, Train_accy 66.34, Test_accy 63.88:  43%|████▎     | 13/30 [1:01:12<1:12:08, 254.63s/it]Task 2, Epoch 14/30 => Loss 1.502, Train_accy 66.34, Test_accy 63.88:  47%|████▋     | 14/30 [1:01:12<1:07:24, 252.80s/it]Task 2, Epoch 15/30 => Loss 1.487, Train_accy 66.51, Test_accy 63.83:  47%|████▋     | 14/30 [1:05:22<1:07:24, 252.80s/it]Task 2, Epoch 15/30 => Loss 1.487, Train_accy 66.51, Test_accy 63.83:  50%|█████     | 15/30 [1:05:22<1:02:58, 251.93s/it]Task 2, Epoch 16/30 => Loss 1.478, Train_accy 66.73, Test_accy 64.12:  50%|█████     | 15/30 [1:09:30<1:02:58, 251.93s/it]Task 2, Epoch 16/30 => Loss 1.478, Train_accy 66.73, Test_accy 64.12:  53%|█████▎    | 16/30 [1:09:30<58:29, 250.66s/it]  Task 2, Epoch 17/30 => Loss 1.487, Train_accy 66.76, Test_accy 63.61:  53%|█████▎    | 16/30 [1:13:37<58:29, 250.66s/it]Task 2, Epoch 17/30 => Loss 1.487, Train_accy 66.76, Test_accy 63.61:  57%|█████▋    | 17/30 [1:13:37<54:05, 249.64s/it]Task 2, Epoch 18/30 => Loss 1.482, Train_accy 66.59, Test_accy 63.76:  57%|█████▋    | 17/30 [1:17:45<54:05, 249.64s/it]Task 2, Epoch 18/30 => Loss 1.482, Train_accy 66.59, Test_accy 63.76:  60%|██████    | 18/30 [1:17:45<49:50, 249.23s/it]Task 2, Epoch 19/30 => Loss 1.466, Train_accy 67.24, Test_accy 63.90:  60%|██████    | 18/30 [1:21:53<49:50, 249.23s/it]Task 2, Epoch 19/30 => Loss 1.466, Train_accy 67.24, Test_accy 63.90:  63%|██████▎   | 19/30 [1:21:53<45:36, 248.73s/it]Task 2, Epoch 20/30 => Loss 1.460, Train_accy 66.93, Test_accy 63.91:  63%|██████▎   | 19/30 [1:26:02<45:36, 248.73s/it]Task 2, Epoch 20/30 => Loss 1.460, Train_accy 66.93, Test_accy 63.91:  67%|██████▋   | 20/30 [1:26:02<41:26, 248.67s/it]Task 2, Epoch 21/30 => Loss 1.463, Train_accy 67.11, Test_accy 63.62:  67%|██████▋   | 20/30 [1:30:11<41:26, 248.67s/it]Task 2, Epoch 21/30 => Loss 1.463, Train_accy 67.11, Test_accy 63.62:  70%|███████   | 21/30 [1:30:11<37:19, 248.83s/it]Task 2, Epoch 22/30 => Loss 1.459, Train_accy 67.19, Test_accy 63.94:  70%|███████   | 21/30 [1:34:20<37:19, 248.83s/it]Task 2, Epoch 22/30 => Loss 1.459, Train_accy 67.19, Test_accy 63.94:  73%|███████▎  | 22/30 [1:34:20<33:12, 249.04s/it]Task 2, Epoch 23/30 => Loss 1.455, Train_accy 67.22, Test_accy 63.67:  73%|███████▎  | 22/30 [1:38:29<33:12, 249.04s/it]Task 2, Epoch 23/30 => Loss 1.455, Train_accy 67.22, Test_accy 63.67:  77%|███████▋  | 23/30 [1:38:29<29:01, 248.81s/it]Task 2, Epoch 24/30 => Loss 1.452, Train_accy 67.26, Test_accy 63.63:  77%|███████▋  | 23/30 [1:42:38<29:01, 248.81s/it]Task 2, Epoch 24/30 => Loss 1.452, Train_accy 67.26, Test_accy 63.63:  80%|████████  | 24/30 [1:42:38<24:53, 248.99s/it]Task 2, Epoch 25/30 => Loss 1.439, Train_accy 67.36, Test_accy 63.97:  80%|████████  | 24/30 [1:46:48<24:53, 248.99s/it]Task 2, Epoch 25/30 => Loss 1.439, Train_accy 67.36, Test_accy 63.97:  83%|████████▎ | 25/30 [1:46:48<20:47, 249.40s/it]Task 2, Epoch 26/30 => Loss 1.451, Train_accy 67.10, Test_accy 63.53:  83%|████████▎ | 25/30 [1:50:57<20:47, 249.40s/it]Task 2, Epoch 26/30 => Loss 1.451, Train_accy 67.10, Test_accy 63.53:  87%|████████▋ | 26/30 [1:50:57<16:36, 249.23s/it]Task 2, Epoch 27/30 => Loss 1.443, Train_accy 67.47, Test_accy 63.57:  87%|████████▋ | 26/30 [1:55:05<16:36, 249.23s/it]Task 2, Epoch 27/30 => Loss 1.443, Train_accy 67.47, Test_accy 63.57:  90%|█████████ | 27/30 [1:55:05<12:26, 248.87s/it]Task 2, Epoch 28/30 => Loss 1.424, Train_accy 67.81, Test_accy 63.56:  90%|█████████ | 27/30 [1:59:13<12:26, 248.87s/it]Task 2, Epoch 28/30 => Loss 1.424, Train_accy 67.81, Test_accy 63.56:  93%|█████████▎| 28/30 [1:59:13<08:17, 248.68s/it]Task 2, Epoch 29/30 => Loss 1.441, Train_accy 67.48, Test_accy 63.54:  93%|█████████▎| 28/30 [2:03:22<08:17, 248.68s/it]Task 2, Epoch 29/30 => Loss 1.441, Train_accy 67.48, Test_accy 63.54:  97%|█████████▋| 29/30 [2:03:22<04:08, 248.62s/it]Task 2, Epoch 30/30 => Loss 1.440, Train_accy 67.45, Test_accy 63.54:  97%|█████████▋| 29/30 [2:07:30<04:08, 248.62s/it]Task 2, Epoch 30/30 => Loss 1.440, Train_accy 67.45, Test_accy 63.54: 100%|██████████| 30/30 [2:07:30<00:00, 248.52s/it]Task 2, Epoch 30/30 => Loss 1.440, Train_accy 67.45, Test_accy 63.54: 100%|██████████| 30/30 [2:07:30<00:00, 255.02s/it]
2024-06-13 15:04:44,920 [lora_prompt.py] => Task 2, Epoch 30/30 => Loss 1.440, Train_accy 67.45, Test_accy 63.54
2024-06-13 15:16:53,236 [lora_prompt.py] => Exemplar size: 0
2024-06-13 15:16:53,237 [trainer.py] => CNN: {'total': 70.46, '00-344': 77.18, '345-689': 57.44, '690-1034': 75.25, 'old': 66.99, 'new': 75.25}
2024-06-13 15:16:53,237 [trainer.py] => CNN top1 curve: [78.1, 67.47, 70.46]
2024-06-13 15:16:53,243 [trainer.py] => All params: 668051457
2024-06-13 15:16:53,248 [trainer.py] => Trainable params: 204800
2024-06-13 15:16:53,249 [lora_prompt.py] => Learning on 1035-1380
Parameters to be updated: {'lora_image_model_pool.3.base_model.model.transformer.resblocks.1.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.7.attn.out_proj.lora_A.weight', 'classifier_pool.3.ctx', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.8.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.3.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.5.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.9.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.2.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.11.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.1.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.4.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.6.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.6.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.9.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.8.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.0.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.3.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.10.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.11.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.7.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.5.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.0.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.10.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.4.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.2.attn.out_proj.lora_A.weight'},count:204800
  0%|          | 0/30 [00:00<?, ?it/s]Task 3, Epoch 1/30 => Loss 4.696, Train_accy 11.50, Test_accy 36.78:   0%|          | 0/30 [09:06<?, ?it/s]Task 3, Epoch 1/30 => Loss 4.696, Train_accy 11.50, Test_accy 36.78:   3%|▎         | 1/30 [09:06<4:24:22, 546.97s/it]Task 3, Epoch 2/30 => Loss 4.433, Train_accy 14.05, Test_accy 35.99:   3%|▎         | 1/30 [18:12<4:24:22, 546.97s/it]Task 3, Epoch 2/30 => Loss 4.433, Train_accy 14.05, Test_accy 35.99:   7%|▋         | 2/30 [18:12<4:14:56, 546.29s/it]Task 3, Epoch 3/30 => Loss 4.340, Train_accy 15.08, Test_accy 36.11:   7%|▋         | 2/30 [27:14<4:14:56, 546.29s/it]Task 3, Epoch 3/30 => Loss 4.340, Train_accy 15.08, Test_accy 36.11:  10%|█         | 3/30 [27:14<4:04:53, 544.22s/it]Task 3, Epoch 4/30 => Loss 4.284, Train_accy 15.64, Test_accy 35.39:  10%|█         | 3/30 [36:17<4:04:53, 544.22s/it]Task 3, Epoch 4/30 => Loss 4.284, Train_accy 15.64, Test_accy 35.39:  13%|█▎        | 4/30 [36:17<3:55:33, 543.60s/it]Task 3, Epoch 5/30 => Loss 4.231, Train_accy 16.19, Test_accy 35.83:  13%|█▎        | 4/30 [45:21<3:55:33, 543.60s/it]Task 3, Epoch 5/30 => Loss 4.231, Train_accy 16.19, Test_accy 35.83:  17%|█▋        | 5/30 [45:21<3:46:34, 543.80s/it]Task 3, Epoch 6/30 => Loss 4.196, Train_accy 16.69, Test_accy 35.52:  17%|█▋        | 5/30 [54:24<3:46:34, 543.80s/it]Task 3, Epoch 6/30 => Loss 4.196, Train_accy 16.69, Test_accy 35.52:  20%|██        | 6/30 [54:24<3:37:26, 543.62s/it]Task 3, Epoch 7/30 => Loss 4.167, Train_accy 16.99, Test_accy 34.89:  20%|██        | 6/30 [1:03:28<3:37:26, 543.62s/it]Task 3, Epoch 7/30 => Loss 4.167, Train_accy 16.99, Test_accy 34.89:  23%|██▎       | 7/30 [1:03:28<3:28:24, 543.65s/it]Task 3, Epoch 8/30 => Loss 4.145, Train_accy 17.33, Test_accy 35.87:  23%|██▎       | 7/30 [1:12:30<3:28:24, 543.65s/it]Task 3, Epoch 8/30 => Loss 4.145, Train_accy 17.33, Test_accy 35.87:  27%|██▋       | 8/30 [1:12:30<3:19:08, 543.10s/it]Task 3, Epoch 9/30 => Loss 4.131, Train_accy 17.38, Test_accy 34.62:  27%|██▋       | 8/30 [1:21:32<3:19:08, 543.10s/it]Task 3, Epoch 9/30 => Loss 4.131, Train_accy 17.38, Test_accy 34.62:  30%|███       | 9/30 [1:21:32<3:10:02, 542.98s/it]Task 3, Epoch 10/30 => Loss 4.110, Train_accy 17.68, Test_accy 35.28:  30%|███       | 9/30 [1:30:36<3:10:02, 542.98s/it]Task 3, Epoch 10/30 => Loss 4.110, Train_accy 17.68, Test_accy 35.28:  33%|███▎      | 10/30 [1:30:36<3:01:04, 543.22s/it]Task 3, Epoch 11/30 => Loss 4.097, Train_accy 17.96, Test_accy 35.09:  33%|███▎      | 10/30 [1:39:42<3:01:04, 543.22s/it]Task 3, Epoch 11/30 => Loss 4.097, Train_accy 17.96, Test_accy 35.09:  37%|███▋      | 11/30 [1:39:42<2:52:15, 543.95s/it]Task 3, Epoch 12/30 => Loss 4.078, Train_accy 18.39, Test_accy 35.25:  37%|███▋      | 11/30 [1:48:44<2:52:15, 543.95s/it]Task 3, Epoch 12/30 => Loss 4.078, Train_accy 18.39, Test_accy 35.25:  40%|████      | 12/30 [1:48:44<2:42:58, 543.26s/it]Task 3, Epoch 13/30 => Loss 4.068, Train_accy 18.50, Test_accy 35.20:  40%|████      | 12/30 [1:57:47<2:42:58, 543.26s/it]Task 3, Epoch 13/30 => Loss 4.068, Train_accy 18.50, Test_accy 35.20:  43%|████▎     | 13/30 [1:57:47<2:33:58, 543.45s/it]Task 3, Epoch 14/30 => Loss 4.052, Train_accy 18.63, Test_accy 35.26:  43%|████▎     | 13/30 [2:06:52<2:33:58, 543.45s/it]Task 3, Epoch 14/30 => Loss 4.052, Train_accy 18.63, Test_accy 35.26:  47%|████▋     | 14/30 [2:06:52<2:24:58, 543.66s/it]Task 3, Epoch 15/30 => Loss 4.037, Train_accy 18.67, Test_accy 35.25:  47%|████▋     | 14/30 [2:15:57<2:24:58, 543.66s/it]Task 3, Epoch 15/30 => Loss 4.037, Train_accy 18.67, Test_accy 35.25:  50%|█████     | 15/30 [2:15:57<2:16:03, 544.25s/it]Task 3, Epoch 16/30 => Loss 4.027, Train_accy 18.94, Test_accy 35.56:  50%|█████     | 15/30 [2:25:00<2:16:03, 544.25s/it]Task 3, Epoch 16/30 => Loss 4.027, Train_accy 18.94, Test_accy 35.56:  53%|█████▎    | 16/30 [2:25:00<2:06:54, 543.89s/it]Task 3, Epoch 17/30 => Loss 4.015, Train_accy 19.17, Test_accy 35.10:  53%|█████▎    | 16/30 [2:34:06<2:06:54, 543.89s/it]Task 3, Epoch 17/30 => Loss 4.015, Train_accy 19.17, Test_accy 35.10:  57%|█████▋    | 17/30 [2:34:06<1:57:56, 544.37s/it]Task 3, Epoch 18/30 => Loss 4.001, Train_accy 19.28, Test_accy 34.93:  57%|█████▋    | 17/30 [2:43:12<1:57:56, 544.37s/it]Task 3, Epoch 18/30 => Loss 4.001, Train_accy 19.28, Test_accy 34.93:  60%|██████    | 18/30 [2:43:12<1:48:58, 544.88s/it]Task 3, Epoch 19/30 => Loss 3.997, Train_accy 19.16, Test_accy 35.54:  60%|██████    | 18/30 [2:52:17<1:48:58, 544.88s/it]Task 3, Epoch 19/30 => Loss 3.997, Train_accy 19.16, Test_accy 35.54:  63%|██████▎   | 19/30 [2:52:17<1:39:53, 544.88s/it]Task 3, Epoch 20/30 => Loss 3.979, Train_accy 19.63, Test_accy 34.95:  63%|██████▎   | 19/30 [3:01:23<1:39:53, 544.88s/it]Task 3, Epoch 20/30 => Loss 3.979, Train_accy 19.63, Test_accy 34.95:  67%|██████▋   | 20/30 [3:01:23<1:30:51, 545.19s/it]Task 3, Epoch 21/30 => Loss 3.962, Train_accy 19.95, Test_accy 35.84:  67%|██████▋   | 20/30 [3:10:26<1:30:51, 545.19s/it]Task 3, Epoch 21/30 => Loss 3.962, Train_accy 19.95, Test_accy 35.84:  70%|███████   | 21/30 [3:10:26<1:21:42, 544.77s/it]Task 3, Epoch 22/30 => Loss 3.957, Train_accy 19.95, Test_accy 35.48:  70%|███████   | 21/30 [3:19:31<1:21:42, 544.77s/it]Task 3, Epoch 22/30 => Loss 3.957, Train_accy 19.95, Test_accy 35.48:  73%|███████▎  | 22/30 [3:19:31<1:12:38, 544.86s/it]Task 3, Epoch 23/30 => Loss 3.948, Train_accy 20.16, Test_accy 35.24:  73%|███████▎  | 22/30 [3:28:34<1:12:38, 544.86s/it]Task 3, Epoch 23/30 => Loss 3.948, Train_accy 20.16, Test_accy 35.24:  77%|███████▋  | 23/30 [3:28:34<1:03:29, 544.21s/it]Task 3, Epoch 24/30 => Loss 3.948, Train_accy 20.22, Test_accy 35.38:  77%|███████▋  | 23/30 [3:37:37<1:03:29, 544.21s/it]Task 3, Epoch 24/30 => Loss 3.948, Train_accy 20.22, Test_accy 35.38:  80%|████████  | 24/30 [3:37:37<54:22, 543.76s/it]  Task 3, Epoch 25/30 => Loss 3.929, Train_accy 20.35, Test_accy 35.38:  80%|████████  | 24/30 [3:46:44<54:22, 543.76s/it]Task 3, Epoch 25/30 => Loss 3.929, Train_accy 20.35, Test_accy 35.38:  83%|████████▎ | 25/30 [3:46:44<45:23, 544.68s/it]Task 3, Epoch 26/30 => Loss 3.926, Train_accy 20.47, Test_accy 35.30:  83%|████████▎ | 25/30 [3:55:53<45:23, 544.68s/it]Task 3, Epoch 26/30 => Loss 3.926, Train_accy 20.47, Test_accy 35.30:  87%|████████▋ | 26/30 [3:55:53<36:24, 546.19s/it]Task 3, Epoch 27/30 => Loss 3.922, Train_accy 20.43, Test_accy 35.43:  87%|████████▋ | 26/30 [4:05:03<36:24, 546.19s/it]Task 3, Epoch 27/30 => Loss 3.922, Train_accy 20.43, Test_accy 35.43:  90%|█████████ | 27/30 [4:05:03<27:22, 547.34s/it]Task 3, Epoch 28/30 => Loss 3.907, Train_accy 20.78, Test_accy 35.69:  90%|█████████ | 27/30 [4:14:15<27:22, 547.34s/it]Task 3, Epoch 28/30 => Loss 3.907, Train_accy 20.78, Test_accy 35.69:  93%|█████████▎| 28/30 [4:14:15<18:17, 548.77s/it]Task 3, Epoch 29/30 => Loss 3.914, Train_accy 20.63, Test_accy 35.72:  93%|█████████▎| 28/30 [4:23:27<18:17, 548.77s/it]Task 3, Epoch 29/30 => Loss 3.914, Train_accy 20.63, Test_accy 35.72:  97%|█████████▋| 29/30 [4:23:27<09:09, 549.60s/it]Task 3, Epoch 30/30 => Loss 3.902, Train_accy 20.90, Test_accy 35.73:  97%|█████████▋| 29/30 [4:32:36<09:09, 549.60s/it]Task 3, Epoch 30/30 => Loss 3.902, Train_accy 20.90, Test_accy 35.73: 100%|██████████| 30/30 [4:32:36<00:00, 549.44s/it]Task 3, Epoch 30/30 => Loss 3.902, Train_accy 20.90, Test_accy 35.73: 100%|██████████| 30/30 [4:32:36<00:00, 545.22s/it]
2024-06-13 19:49:30,655 [lora_prompt.py] => Task 3, Epoch 30/30 => Loss 3.902, Train_accy 20.90, Test_accy 35.73
2024-06-13 20:14:19,695 [lora_prompt.py] => Exemplar size: 0
2024-06-13 20:14:19,696 [trainer.py] => CNN: {'total': 53.86, '00-344': 77.05, '345-689': 57.43, '690-1034': 75.21, '1035-1379': 37.22, 'old': 70.4, 'new': 37.22}
2024-06-13 20:14:19,696 [trainer.py] => CNN top1 curve: [78.1, 67.47, 70.46, 53.86]
2024-06-13 20:14:19,702 [trainer.py] => All params: 668051457
2024-06-13 20:14:19,707 [trainer.py] => Trainable params: 204800
2024-06-13 20:14:19,707 [lora_prompt.py] => Learning on 1380-1725
Parameters to be updated: {'lora_image_model_pool.4.base_model.model.transformer.resblocks.4.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.11.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.9.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.7.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.10.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.6.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.8.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.2.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.6.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.0.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.10.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.3.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.2.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.5.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.5.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.7.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.1.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.4.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.3.attn.out_proj.lora_A.weight', 'classifier_pool.4.ctx', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.8.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.9.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.11.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.1.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.0.attn.out_proj.lora_B.weight'},count:204800
  0%|          | 0/30 [00:00<?, ?it/s]Task 4, Epoch 1/30 => Loss 0.970, Train_accy 76.98, Test_accy 54.35:   0%|          | 0/30 [10:46<?, ?it/s]Task 4, Epoch 1/30 => Loss 0.970, Train_accy 76.98, Test_accy 54.35:   3%|▎         | 1/30 [10:46<5:12:21, 646.27s/it]Task 4, Epoch 2/30 => Loss 0.921, Train_accy 77.80, Test_accy 54.18:   3%|▎         | 1/30 [21:35<5:12:21, 646.27s/it]Task 4, Epoch 2/30 => Loss 0.921, Train_accy 77.80, Test_accy 54.18:   7%|▋         | 2/30 [21:35<5:02:23, 647.99s/it]Task 4, Epoch 3/30 => Loss 0.907, Train_accy 78.09, Test_accy 54.10:   7%|▋         | 2/30 [32:29<5:02:23, 647.99s/it]Task 4, Epoch 3/30 => Loss 0.907, Train_accy 78.09, Test_accy 54.10:  10%|█         | 3/30 [32:29<4:52:52, 650.85s/it]Task 4, Epoch 4/30 => Loss 0.898, Train_accy 78.31, Test_accy 54.19:  10%|█         | 3/30 [43:20<4:52:52, 650.85s/it]Task 4, Epoch 4/30 => Loss 0.898, Train_accy 78.31, Test_accy 54.19:  13%|█▎        | 4/30 [43:20<4:41:56, 650.63s/it]Task 4, Epoch 5/30 => Loss 0.892, Train_accy 78.19, Test_accy 54.10:  13%|█▎        | 4/30 [54:11<4:41:56, 650.63s/it]Task 4, Epoch 5/30 => Loss 0.892, Train_accy 78.19, Test_accy 54.10:  17%|█▋        | 5/30 [54:11<4:31:15, 651.03s/it]Task 4, Epoch 6/30 => Loss 0.882, Train_accy 78.56, Test_accy 54.08:  17%|█▋        | 5/30 [1:05:04<4:31:15, 651.03s/it]Task 4, Epoch 6/30 => Loss 0.882, Train_accy 78.56, Test_accy 54.08:  20%|██        | 6/30 [1:05:04<4:20:42, 651.78s/it]/home/dd/miniconda3/envs/pyspompts/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
./logs/reproduce_1993_lora_prompt_lora_prompt_clip_domainnet_345_345_2024-06-14-09:52:21
2024-06-14 09:52:21,030 [trainer.py] => config: configs/lora-prompt/domainnet.json
2024-06-14 09:52:21,030 [trainer.py] => prefix: reproduce
2024-06-14 09:52:21,030 [trainer.py] => dataset: domainnet
2024-06-14 09:52:21,030 [trainer.py] => data_path: /home/dd/s-prompts/data/domainnet/
2024-06-14 09:52:21,030 [trainer.py] => task_name: ['clipart', 'infograph', 'painting', 'quickdraw', 'real', 'sketch']
2024-06-14 09:52:21,030 [trainer.py] => memory_size: 0
2024-06-14 09:52:21,030 [trainer.py] => memory_per_class: 0
2024-06-14 09:52:21,030 [trainer.py] => fixed_memory: True
2024-06-14 09:52:21,030 [trainer.py] => shuffle: False
2024-06-14 09:52:21,031 [trainer.py] => init_cls: 345
2024-06-14 09:52:21,031 [trainer.py] => increment: 345
2024-06-14 09:52:21,031 [trainer.py] => model_name: lora_prompt
2024-06-14 09:52:21,031 [trainer.py] => net_type: lora_prompt_clip
2024-06-14 09:52:21,031 [trainer.py] => embd_dim: 768
2024-06-14 09:52:21,031 [trainer.py] => prompt_length: 10
2024-06-14 09:52:21,031 [trainer.py] => total_sessions: 6
2024-06-14 09:52:21,031 [trainer.py] => device: [device(type='cuda', index=1)]
2024-06-14 09:52:21,031 [trainer.py] => seed: 1993
2024-06-14 09:52:21,031 [trainer.py] => EPSILON: 1e-08
2024-06-14 09:52:21,031 [trainer.py] => init_epoch: 30
2024-06-14 09:52:21,031 [trainer.py] => init_lr: 0.01
2024-06-14 09:52:21,031 [trainer.py] => init_lr_decay: 0.1
2024-06-14 09:52:21,031 [trainer.py] => init_weight_decay: 0.0005
2024-06-14 09:52:21,031 [trainer.py] => epochs: 30
2024-06-14 09:52:21,031 [trainer.py] => lrate: 0.01
2024-06-14 09:52:21,031 [trainer.py] => lrate_decay: 0.1
2024-06-14 09:52:21,031 [trainer.py] => batch_size: 128
2024-06-14 09:52:21,031 [trainer.py] => weight_decay: 0.0002
2024-06-14 09:52:21,031 [trainer.py] => num_workers: 16
2024-06-14 09:52:22,414 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069]
/home/dd/miniconda3/envs/pyspompts/lib/python3.8/site-packages/peft/tuners/lora.py:173: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.
  warnings.warn(
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
2024-06-14 09:53:53,754 [trainer.py] => All params: 668051457
2024-06-14 09:53:53,760 [trainer.py] => Trainable params: 150895617
2024-06-14 09:53:53,760 [lora_prompt.py] => Learning on 0-345
Parameters to be updated: {'lora_image_model_pool.0.base_model.model.transformer.resblocks.10.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.1.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.8.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.6.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.6.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.11.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.8.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.1.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.11.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.3.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.3.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.0.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.10.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.4.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.7.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.9.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.5.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.0.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.7.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.2.attn.out_proj.lora_B.weight', 'classifier_pool.0.ctx', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.2.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.4.attn.out_proj.lora_A.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.5.attn.out_proj.lora_B.weight', 'lora_image_model_pool.0.base_model.model.transformer.resblocks.9.attn.out_proj.lora_B.weight'},count:204800
  0%|          | 0/30 [00:00<?, ?it/s]Task 0, Epoch 1/30 => Loss 1.853, Train_accy 60.31, Test_accy 74.51:   0%|          | 0/30 [02:07<?, ?it/s]Task 0, Epoch 1/30 => Loss 1.853, Train_accy 60.31, Test_accy 74.51:   3%|▎         | 1/30 [02:07<1:01:27, 127.16s/it]Task 0, Epoch 2/30 => Loss 1.739, Train_accy 62.08, Test_accy 75.42:   3%|▎         | 1/30 [04:15<1:01:27, 127.16s/it]Task 0, Epoch 2/30 => Loss 1.739, Train_accy 62.08, Test_accy 75.42:   7%|▋         | 2/30 [04:15<59:38, 127.80s/it]  Task 0, Epoch 3/30 => Loss 1.696, Train_accy 62.85, Test_accy 75.94:   7%|▋         | 2/30 [06:26<59:38, 127.80s/it]Task 0, Epoch 3/30 => Loss 1.696, Train_accy 62.85, Test_accy 75.94:  10%|█         | 3/30 [06:26<58:13, 129.38s/it]Task 0, Epoch 4/30 => Loss 1.672, Train_accy 63.22, Test_accy 76.38:  10%|█         | 3/30 [08:36<58:13, 129.38s/it]Task 0, Epoch 4/30 => Loss 1.672, Train_accy 63.22, Test_accy 76.38:  13%|█▎        | 4/30 [08:36<56:06, 129.49s/it]Task 0, Epoch 5/30 => Loss 1.631, Train_accy 63.82, Test_accy 76.47:  13%|█▎        | 4/30 [10:45<56:06, 129.49s/it]Task 0, Epoch 5/30 => Loss 1.631, Train_accy 63.82, Test_accy 76.47:  17%|█▋        | 5/30 [10:45<53:58, 129.53s/it]Task 0, Epoch 6/30 => Loss 1.626, Train_accy 64.19, Test_accy 76.88:  17%|█▋        | 5/30 [12:55<53:58, 129.53s/it]Task 0, Epoch 6/30 => Loss 1.626, Train_accy 64.19, Test_accy 76.88:  20%|██        | 6/30 [12:55<51:48, 129.51s/it]Task 0, Epoch 7/30 => Loss 1.634, Train_accy 63.94, Test_accy 77.16:  20%|██        | 6/30 [15:06<51:48, 129.51s/it]Task 0, Epoch 7/30 => Loss 1.634, Train_accy 63.94, Test_accy 77.16:  23%|██▎       | 7/30 [15:06<49:52, 130.09s/it]Task 0, Epoch 8/30 => Loss 1.613, Train_accy 64.18, Test_accy 77.10:  23%|██▎       | 7/30 [17:17<49:52, 130.09s/it]Task 0, Epoch 8/30 => Loss 1.613, Train_accy 64.18, Test_accy 77.10:  27%|██▋       | 8/30 [17:17<47:50, 130.47s/it]Task 0, Epoch 9/30 => Loss 1.589, Train_accy 64.69, Test_accy 77.28:  27%|██▋       | 8/30 [19:27<47:50, 130.47s/it]Task 0, Epoch 9/30 => Loss 1.589, Train_accy 64.69, Test_accy 77.28:  30%|███       | 9/30 [19:27<45:35, 130.28s/it]Task 0, Epoch 10/30 => Loss 1.591, Train_accy 64.48, Test_accy 77.10:  30%|███       | 9/30 [21:37<45:35, 130.28s/it]Task 0, Epoch 10/30 => Loss 1.591, Train_accy 64.48, Test_accy 77.10:  33%|███▎      | 10/30 [21:37<43:20, 130.01s/it]Task 0, Epoch 11/30 => Loss 1.580, Train_accy 64.92, Test_accy 77.25:  33%|███▎      | 10/30 [23:46<43:20, 130.01s/it]Task 0, Epoch 11/30 => Loss 1.580, Train_accy 64.92, Test_accy 77.25:  37%|███▋      | 11/30 [23:46<41:07, 129.85s/it]Task 0, Epoch 12/30 => Loss 1.558, Train_accy 65.23, Test_accy 77.40:  37%|███▋      | 11/30 [25:55<41:07, 129.85s/it]Task 0, Epoch 12/30 => Loss 1.558, Train_accy 65.23, Test_accy 77.40:  40%|████      | 12/30 [25:55<38:49, 129.43s/it]Task 0, Epoch 13/30 => Loss 1.562, Train_accy 65.06, Test_accy 77.81:  40%|████      | 12/30 [28:05<38:49, 129.43s/it]Task 0, Epoch 13/30 => Loss 1.562, Train_accy 65.06, Test_accy 77.81:  43%|████▎     | 13/30 [28:05<36:43, 129.65s/it]Task 0, Epoch 14/30 => Loss 1.553, Train_accy 65.59, Test_accy 77.56:  43%|████▎     | 13/30 [30:15<36:43, 129.65s/it]Task 0, Epoch 14/30 => Loss 1.553, Train_accy 65.59, Test_accy 77.56:  47%|████▋     | 14/30 [30:15<34:34, 129.67s/it]Task 0, Epoch 15/30 => Loss 1.534, Train_accy 65.81, Test_accy 77.49:  47%|████▋     | 14/30 [32:24<34:34, 129.67s/it]Task 0, Epoch 15/30 => Loss 1.534, Train_accy 65.81, Test_accy 77.49:  50%|█████     | 15/30 [32:24<32:25, 129.68s/it]Task 0, Epoch 16/30 => Loss 1.543, Train_accy 65.63, Test_accy 77.95:  50%|█████     | 15/30 [34:34<32:25, 129.68s/it]Task 0, Epoch 16/30 => Loss 1.543, Train_accy 65.63, Test_accy 77.95:  53%|█████▎    | 16/30 [34:34<30:13, 129.55s/it]Task 0, Epoch 17/30 => Loss 1.538, Train_accy 65.58, Test_accy 77.60:  53%|█████▎    | 16/30 [36:42<30:13, 129.55s/it]Task 0, Epoch 17/30 => Loss 1.538, Train_accy 65.58, Test_accy 77.60:  57%|█████▋    | 17/30 [36:42<28:00, 129.30s/it]Task 0, Epoch 18/30 => Loss 1.536, Train_accy 65.77, Test_accy 77.83:  57%|█████▋    | 17/30 [38:52<28:00, 129.30s/it]Task 0, Epoch 18/30 => Loss 1.536, Train_accy 65.77, Test_accy 77.83:  60%|██████    | 18/30 [38:52<25:53, 129.49s/it]Task 0, Epoch 19/30 => Loss 1.513, Train_accy 66.30, Test_accy 77.67:  60%|██████    | 18/30 [41:02<25:53, 129.49s/it]Task 0, Epoch 19/30 => Loss 1.513, Train_accy 66.30, Test_accy 77.67:  63%|██████▎   | 19/30 [41:02<23:45, 129.56s/it]Task 0, Epoch 20/30 => Loss 1.535, Train_accy 65.91, Test_accy 77.87:  63%|██████▎   | 19/30 [43:12<23:45, 129.56s/it]Task 0, Epoch 20/30 => Loss 1.535, Train_accy 65.91, Test_accy 77.87:  67%|██████▋   | 20/30 [43:12<21:38, 129.84s/it]Task 0, Epoch 21/30 => Loss 1.514, Train_accy 66.25, Test_accy 77.99:  67%|██████▋   | 20/30 [45:24<21:38, 129.84s/it]Task 0, Epoch 21/30 => Loss 1.514, Train_accy 66.25, Test_accy 77.99:  70%|███████   | 21/30 [45:24<19:33, 130.40s/it]Task 0, Epoch 22/30 => Loss 1.507, Train_accy 66.33, Test_accy 77.75:  70%|███████   | 21/30 [47:36<19:33, 130.40s/it]Task 0, Epoch 22/30 => Loss 1.507, Train_accy 66.33, Test_accy 77.75:  73%|███████▎  | 22/30 [47:36<17:26, 130.81s/it]Task 0, Epoch 23/30 => Loss 1.510, Train_accy 66.21, Test_accy 78.07:  73%|███████▎  | 22/30 [49:47<17:26, 130.81s/it]Task 0, Epoch 23/30 => Loss 1.510, Train_accy 66.21, Test_accy 78.07:  77%|███████▋  | 23/30 [49:47<15:15, 130.82s/it]Task 0, Epoch 24/30 => Loss 1.492, Train_accy 66.43, Test_accy 77.82:  77%|███████▋  | 23/30 [52:00<15:15, 130.82s/it]Task 0, Epoch 24/30 => Loss 1.492, Train_accy 66.43, Test_accy 77.82:  80%|████████  | 24/30 [52:00<13:09, 131.50s/it]Task 0, Epoch 25/30 => Loss 1.498, Train_accy 66.34, Test_accy 78.12:  80%|████████  | 24/30 [54:11<13:09, 131.50s/it]Task 0, Epoch 25/30 => Loss 1.498, Train_accy 66.34, Test_accy 78.12:  83%|████████▎ | 25/30 [54:11<10:57, 131.50s/it]Task 0, Epoch 26/30 => Loss 1.490, Train_accy 66.28, Test_accy 78.17:  83%|████████▎ | 25/30 [56:24<10:57, 131.50s/it]Task 0, Epoch 26/30 => Loss 1.490, Train_accy 66.28, Test_accy 78.17:  87%|████████▋ | 26/30 [56:24<08:47, 131.84s/it]Task 0, Epoch 27/30 => Loss 1.475, Train_accy 66.78, Test_accy 78.10:  87%|████████▋ | 26/30 [58:35<08:47, 131.84s/it]Task 0, Epoch 27/30 => Loss 1.475, Train_accy 66.78, Test_accy 78.10:  90%|█████████ | 27/30 [58:35<06:34, 131.59s/it]Task 0, Epoch 28/30 => Loss 1.488, Train_accy 66.63, Test_accy 78.10:  90%|█████████ | 27/30 [1:00:47<06:34, 131.59s/it]Task 0, Epoch 28/30 => Loss 1.488, Train_accy 66.63, Test_accy 78.10:  93%|█████████▎| 28/30 [1:00:47<04:23, 131.86s/it]Task 0, Epoch 29/30 => Loss 1.497, Train_accy 66.39, Test_accy 78.12:  93%|█████████▎| 28/30 [1:03:00<04:23, 131.86s/it]Task 0, Epoch 29/30 => Loss 1.497, Train_accy 66.39, Test_accy 78.12:  97%|█████████▋| 29/30 [1:03:00<02:12, 132.17s/it]Task 0, Epoch 30/30 => Loss 1.484, Train_accy 66.71, Test_accy 78.14:  97%|█████████▋| 29/30 [1:05:12<02:12, 132.17s/it]Task 0, Epoch 30/30 => Loss 1.484, Train_accy 66.71, Test_accy 78.14: 100%|██████████| 30/30 [1:05:12<00:00, 132.14s/it]Task 0, Epoch 30/30 => Loss 1.484, Train_accy 66.71, Test_accy 78.14: 100%|██████████| 30/30 [1:05:12<00:00, 130.43s/it]
2024-06-14 10:59:09,066 [lora_prompt.py] => Task 0, Epoch 30/30 => Loss 1.484, Train_accy 66.71, Test_accy 78.14
2024-06-14 11:03:11,606 [lora_prompt.py] => Exemplar size: 0
2024-06-14 11:03:11,606 [trainer.py] => CNN: {'total': 78.1, '00-344': 78.1, 'old': 0, 'new': 78.1}
2024-06-14 11:03:11,606 [trainer.py] => CNN top1 curve: [78.1]
2024-06-14 11:03:11,612 [trainer.py] => All params: 668051457
2024-06-14 11:03:11,618 [trainer.py] => Trainable params: 204800
2024-06-14 11:03:11,618 [lora_prompt.py] => Learning on 345-690
Parameters to be updated: {'lora_image_model_pool.1.base_model.model.transformer.resblocks.4.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.8.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.8.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.5.attn.out_proj.lora_B.weight', 'classifier_pool.1.ctx', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.10.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.7.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.11.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.11.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.1.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.9.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.0.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.6.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.0.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.1.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.6.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.10.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.7.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.2.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.2.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.9.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.4.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.3.attn.out_proj.lora_B.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.5.attn.out_proj.lora_A.weight', 'lora_image_model_pool.1.base_model.model.transformer.resblocks.3.attn.out_proj.lora_A.weight'},count:204800
  0%|          | 0/30 [00:00<?, ?it/s]Task 1, Epoch 1/30 => Loss 2.779, Train_accy 44.85, Test_accy 62.47:   0%|          | 0/30 [02:50<?, ?it/s]Task 1, Epoch 1/30 => Loss 2.779, Train_accy 44.85, Test_accy 62.47:   3%|▎         | 1/30 [02:50<1:22:37, 170.94s/it]Task 1, Epoch 2/30 => Loss 2.620, Train_accy 46.66, Test_accy 62.53:   3%|▎         | 1/30 [05:44<1:22:37, 170.94s/it]Task 1, Epoch 2/30 => Loss 2.620, Train_accy 46.66, Test_accy 62.53:   7%|▋         | 2/30 [05:44<1:20:25, 172.35s/it]Task 1, Epoch 3/30 => Loss 2.561, Train_accy 47.37, Test_accy 62.64:   7%|▋         | 2/30 [08:38<1:20:25, 172.35s/it]Task 1, Epoch 3/30 => Loss 2.561, Train_accy 47.37, Test_accy 62.64:  10%|█         | 3/30 [08:38<1:17:57, 173.26s/it]Task 1, Epoch 4/30 => Loss 2.553, Train_accy 46.99, Test_accy 62.09:  10%|█         | 3/30 [11:33<1:17:57, 173.26s/it]Task 1, Epoch 4/30 => Loss 2.553, Train_accy 46.99, Test_accy 62.09:  13%|█▎        | 4/30 [11:33<1:15:18, 173.79s/it]Task 1, Epoch 5/30 => Loss 2.528, Train_accy 47.62, Test_accy 62.44:  13%|█▎        | 4/30 [14:28<1:15:18, 173.79s/it]Task 1, Epoch 5/30 => Loss 2.528, Train_accy 47.62, Test_accy 62.44:  17%|█▋        | 5/30 [14:28<1:12:39, 174.40s/it]Task 1, Epoch 6/30 => Loss 2.524, Train_accy 47.66, Test_accy 62.20:  17%|█▋        | 5/30 [17:23<1:12:39, 174.40s/it]Task 1, Epoch 6/30 => Loss 2.524, Train_accy 47.66, Test_accy 62.20:  20%|██        | 6/30 [17:23<1:09:46, 174.45s/it]Task 1, Epoch 7/30 => Loss 2.498, Train_accy 47.68, Test_accy 62.18:  20%|██        | 6/30 [20:17<1:09:46, 174.45s/it]Task 1, Epoch 7/30 => Loss 2.498, Train_accy 47.68, Test_accy 62.18:  23%|██▎       | 7/30 [20:17<1:06:54, 174.55s/it]Task 1, Epoch 8/30 => Loss 2.484, Train_accy 48.14, Test_accy 62.41:  23%|██▎       | 7/30 [23:11<1:06:54, 174.55s/it]Task 1, Epoch 8/30 => Loss 2.484, Train_accy 48.14, Test_accy 62.41:  27%|██▋       | 8/30 [23:11<1:03:54, 174.29s/it]Task 1, Epoch 9/30 => Loss 2.467, Train_accy 48.54, Test_accy 62.34:  27%|██▋       | 8/30 [26:03<1:03:54, 174.29s/it]Task 1, Epoch 9/30 => Loss 2.467, Train_accy 48.54, Test_accy 62.34:  30%|███       | 9/30 [26:03<1:00:44, 173.54s/it]Task 1, Epoch 10/30 => Loss 2.474, Train_accy 47.94, Test_accy 62.23:  30%|███       | 9/30 [28:57<1:00:44, 173.54s/it]Task 1, Epoch 10/30 => Loss 2.474, Train_accy 47.94, Test_accy 62.23:  33%|███▎      | 10/30 [28:57<57:50, 173.51s/it] Task 1, Epoch 11/30 => Loss 2.456, Train_accy 48.34, Test_accy 62.69:  33%|███▎      | 10/30 [31:51<57:50, 173.51s/it]Task 1, Epoch 11/30 => Loss 2.456, Train_accy 48.34, Test_accy 62.69:  37%|███▋      | 11/30 [31:51<55:00, 173.70s/it]Task 1, Epoch 12/30 => Loss 2.437, Train_accy 48.87, Test_accy 61.67:  37%|███▋      | 11/30 [34:44<55:00, 173.70s/it]Task 1, Epoch 12/30 => Loss 2.437, Train_accy 48.87, Test_accy 61.67:  40%|████      | 12/30 [34:44<52:06, 173.67s/it]Task 1, Epoch 13/30 => Loss 2.448, Train_accy 48.74, Test_accy 61.83:  40%|████      | 12/30 [37:37<52:06, 173.67s/it]Task 1, Epoch 13/30 => Loss 2.448, Train_accy 48.74, Test_accy 61.83:  43%|████▎     | 13/30 [37:37<49:06, 173.35s/it]Task 1, Epoch 14/30 => Loss 2.447, Train_accy 48.48, Test_accy 61.62:  43%|████▎     | 13/30 [40:30<49:06, 173.35s/it]Task 1, Epoch 14/30 => Loss 2.447, Train_accy 48.48, Test_accy 61.62:  47%|████▋     | 14/30 [40:30<46:13, 173.36s/it]Task 1, Epoch 15/30 => Loss 2.434, Train_accy 48.54, Test_accy 62.49:  47%|████▋     | 14/30 [43:22<46:13, 173.36s/it]Task 1, Epoch 15/30 => Loss 2.434, Train_accy 48.54, Test_accy 62.49:  50%|█████     | 15/30 [43:22<43:12, 172.84s/it]Task 1, Epoch 16/30 => Loss 2.420, Train_accy 48.89, Test_accy 62.33:  50%|█████     | 15/30 [46:14<43:12, 172.84s/it]Task 1, Epoch 16/30 => Loss 2.420, Train_accy 48.89, Test_accy 62.33:  53%|█████▎    | 16/30 [46:14<40:16, 172.64s/it]Task 1, Epoch 17/30 => Loss 2.422, Train_accy 49.02, Test_accy 62.28:  53%|█████▎    | 16/30 [49:08<40:16, 172.64s/it]Task 1, Epoch 17/30 => Loss 2.422, Train_accy 49.02, Test_accy 62.28:  57%|█████▋    | 17/30 [49:08<37:29, 173.02s/it]Task 1, Epoch 18/30 => Loss 2.416, Train_accy 49.11, Test_accy 61.98:  57%|█████▋    | 17/30 [52:00<37:29, 173.02s/it]Task 1, Epoch 18/30 => Loss 2.416, Train_accy 49.11, Test_accy 61.98:  60%|██████    | 18/30 [52:00<34:31, 172.61s/it]Task 1, Epoch 19/30 => Loss 2.412, Train_accy 48.88, Test_accy 62.08:  60%|██████    | 18/30 [54:52<34:31, 172.61s/it]Task 1, Epoch 19/30 => Loss 2.412, Train_accy 48.88, Test_accy 62.08:  63%|██████▎   | 19/30 [54:52<31:38, 172.56s/it]Task 1, Epoch 20/30 => Loss 2.403, Train_accy 49.29, Test_accy 62.00:  63%|██████▎   | 19/30 [57:47<31:38, 172.56s/it]Task 1, Epoch 20/30 => Loss 2.403, Train_accy 49.29, Test_accy 62.00:  67%|██████▋   | 20/30 [57:47<28:51, 173.19s/it]Task 1, Epoch 21/30 => Loss 2.420, Train_accy 48.83, Test_accy 61.77:  67%|██████▋   | 20/30 [1:00:40<28:51, 173.19s/it]Task 1, Epoch 21/30 => Loss 2.420, Train_accy 48.83, Test_accy 61.77:  70%|███████   | 21/30 [1:00:40<25:58, 173.13s/it]Task 1, Epoch 22/30 => Loss 2.393, Train_accy 49.03, Test_accy 61.73:  70%|███████   | 21/30 [1:03:33<25:58, 173.13s/it]Task 1, Epoch 22/30 => Loss 2.393, Train_accy 49.03, Test_accy 61.73:  73%|███████▎  | 22/30 [1:03:33<23:05, 173.23s/it]Task 1, Epoch 23/30 => Loss 2.404, Train_accy 49.13, Test_accy 61.69:  73%|███████▎  | 22/30 [1:06:25<23:05, 173.23s/it]Task 1, Epoch 23/30 => Loss 2.404, Train_accy 49.13, Test_accy 61.69:  77%|███████▋  | 23/30 [1:06:25<20:10, 172.90s/it]Task 1, Epoch 24/30 => Loss 2.402, Train_accy 49.15, Test_accy 62.06:  77%|███████▋  | 23/30 [1:09:18<20:10, 172.90s/it]Task 1, Epoch 24/30 => Loss 2.402, Train_accy 49.15, Test_accy 62.06:  80%|████████  | 24/30 [1:09:18<17:16, 172.71s/it]Task 1, Epoch 25/30 => Loss 2.394, Train_accy 49.34, Test_accy 61.90:  80%|████████  | 24/30 [1:12:11<17:16, 172.71s/it]Task 1, Epoch 25/30 => Loss 2.394, Train_accy 49.34, Test_accy 61.90:  83%|████████▎ | 25/30 [1:12:11<14:24, 172.87s/it]Task 1, Epoch 26/30 => Loss 2.381, Train_accy 49.50, Test_accy 61.92:  83%|████████▎ | 25/30 [1:15:04<14:24, 172.87s/it]Task 1, Epoch 26/30 => Loss 2.381, Train_accy 49.50, Test_accy 61.92:  87%|████████▋ | 26/30 [1:15:04<11:31, 172.86s/it]Task 1, Epoch 27/30 => Loss 2.376, Train_accy 49.36, Test_accy 61.83:  87%|████████▋ | 26/30 [1:17:57<11:31, 172.86s/it]Task 1, Epoch 27/30 => Loss 2.376, Train_accy 49.36, Test_accy 61.83:  90%|█████████ | 27/30 [1:17:57<08:38, 172.97s/it]Task 1, Epoch 28/30 => Loss 2.404, Train_accy 49.14, Test_accy 61.92:  90%|█████████ | 27/30 [1:20:52<08:38, 172.97s/it]Task 1, Epoch 28/30 => Loss 2.404, Train_accy 49.14, Test_accy 61.92:  93%|█████████▎| 28/30 [1:20:52<05:46, 173.47s/it]Task 1, Epoch 29/30 => Loss 2.398, Train_accy 49.25, Test_accy 61.84:  93%|█████████▎| 28/30 [1:23:43<05:46, 173.47s/it]Task 1, Epoch 29/30 => Loss 2.398, Train_accy 49.25, Test_accy 61.84:  97%|█████████▋| 29/30 [1:23:43<02:52, 172.84s/it]Task 1, Epoch 30/30 => Loss 2.386, Train_accy 49.18, Test_accy 61.84:  97%|█████████▋| 29/30 [1:26:38<02:52, 172.84s/it]Task 1, Epoch 30/30 => Loss 2.386, Train_accy 49.18, Test_accy 61.84: 100%|██████████| 30/30 [1:26:38<00:00, 173.62s/it]Task 1, Epoch 30/30 => Loss 2.386, Train_accy 49.18, Test_accy 61.84: 100%|██████████| 30/30 [1:26:38<00:00, 173.30s/it]
2024-06-14 12:29:50,966 [lora_prompt.py] => Task 1, Epoch 30/30 => Loss 2.386, Train_accy 49.18, Test_accy 61.84
2024-06-14 12:37:17,086 [lora_prompt.py] => Exemplar size: 0
2024-06-14 12:37:17,087 [trainer.py] => CNN: {'total': 67.47, '00-344': 77.49, '345-689': 58.07, 'old': 77.49, 'new': 58.07}
2024-06-14 12:37:17,087 [trainer.py] => CNN top1 curve: [78.1, 67.47]
2024-06-14 12:37:17,094 [trainer.py] => All params: 668051457
2024-06-14 12:37:17,101 [trainer.py] => Trainable params: 204800
2024-06-14 12:37:17,101 [lora_prompt.py] => Learning on 690-1035
Parameters to be updated: {'lora_image_model_pool.2.base_model.model.transformer.resblocks.9.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.5.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.7.attn.out_proj.lora_A.weight', 'classifier_pool.2.ctx', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.11.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.10.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.1.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.8.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.2.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.3.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.11.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.5.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.4.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.7.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.8.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.3.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.10.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.6.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.9.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.0.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.6.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.1.attn.out_proj.lora_B.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.2.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.0.attn.out_proj.lora_A.weight', 'lora_image_model_pool.2.base_model.model.transformer.resblocks.4.attn.out_proj.lora_A.weight'},count:204800
  0%|          | 0/30 [00:00<?, ?it/s]Task 2, Epoch 1/30 => Loss 1.779, Train_accy 61.88, Test_accy 65.34:   0%|          | 0/30 [04:16<?, ?it/s]Task 2, Epoch 1/30 => Loss 1.779, Train_accy 61.88, Test_accy 65.34:   3%|▎         | 1/30 [04:16<2:03:54, 256.35s/it]Task 2, Epoch 2/30 => Loss 1.656, Train_accy 63.79, Test_accy 65.00:   3%|▎         | 1/30 [08:33<2:03:54, 256.35s/it]Task 2, Epoch 2/30 => Loss 1.656, Train_accy 63.79, Test_accy 65.00:   7%|▋         | 2/30 [08:33<1:59:54, 256.95s/it]Task 2, Epoch 3/30 => Loss 1.612, Train_accy 64.45, Test_accy 65.39:   7%|▋         | 2/30 [12:50<1:59:54, 256.95s/it]Task 2, Epoch 3/30 => Loss 1.612, Train_accy 64.45, Test_accy 65.39:  10%|█         | 3/30 [12:50<1:55:37, 256.96s/it]Task 2, Epoch 4/30 => Loss 1.572, Train_accy 65.28, Test_accy 64.93:  10%|█         | 3/30 [17:08<1:55:37, 256.96s/it]Task 2, Epoch 4/30 => Loss 1.572, Train_accy 65.28, Test_accy 64.93:  13%|█▎        | 4/30 [17:08<1:51:32, 257.39s/it]Task 2, Epoch 5/30 => Loss 1.568, Train_accy 65.22, Test_accy 64.75:  13%|█▎        | 4/30 [21:25<1:51:32, 257.39s/it]Task 2, Epoch 5/30 => Loss 1.568, Train_accy 65.22, Test_accy 64.75:  17%|█▋        | 5/30 [21:25<1:47:13, 257.33s/it]Task 2, Epoch 6/30 => Loss 1.559, Train_accy 65.38, Test_accy 64.75:  17%|█▋        | 5/30 [25:43<1:47:13, 257.33s/it]Task 2, Epoch 6/30 => Loss 1.559, Train_accy 65.38, Test_accy 64.75:  20%|██        | 6/30 [25:43<1:42:57, 257.40s/it]Task 2, Epoch 7/30 => Loss 1.539, Train_accy 65.61, Test_accy 64.47:  20%|██        | 6/30 [29:58<1:42:57, 257.40s/it]Task 2, Epoch 7/30 => Loss 1.539, Train_accy 65.61, Test_accy 64.47:  23%|██▎       | 7/30 [29:58<1:38:25, 256.77s/it]Task 2, Epoch 8/30 => Loss 1.532, Train_accy 65.79, Test_accy 64.49:  23%|██▎       | 7/30 [34:14<1:38:25, 256.77s/it]Task 2, Epoch 8/30 => Loss 1.532, Train_accy 65.79, Test_accy 64.49:  27%|██▋       | 8/30 [34:14<1:34:02, 256.48s/it]Task 2, Epoch 9/30 => Loss 1.521, Train_accy 65.94, Test_accy 64.07:  27%|██▋       | 8/30 [38:30<1:34:02, 256.48s/it]Task 2, Epoch 9/30 => Loss 1.521, Train_accy 65.94, Test_accy 64.07:  30%|███       | 9/30 [38:30<1:29:39, 256.16s/it]Task 2, Epoch 10/30 => Loss 1.501, Train_accy 66.33, Test_accy 64.69:  30%|███       | 9/30 [42:44<1:29:39, 256.16s/it]Task 2, Epoch 10/30 => Loss 1.501, Train_accy 66.33, Test_accy 64.69:  33%|███▎      | 10/30 [42:44<1:25:13, 255.68s/it]Task 2, Epoch 11/30 => Loss 1.515, Train_accy 66.06, Test_accy 64.47:  33%|███▎      | 10/30 [46:59<1:25:13, 255.68s/it]Task 2, Epoch 11/30 => Loss 1.515, Train_accy 66.06, Test_accy 64.47:  37%|███▋      | 11/30 [46:59<1:20:49, 255.25s/it]Task 2, Epoch 12/30 => Loss 1.504, Train_accy 66.30, Test_accy 64.04:  37%|███▋      | 11/30 [51:11<1:20:49, 255.25s/it]Task 2, Epoch 12/30 => Loss 1.504, Train_accy 66.30, Test_accy 64.04:  40%|████      | 12/30 [51:11<1:16:17, 254.31s/it]Task 2, Epoch 13/30 => Loss 1.488, Train_accy 66.64, Test_accy 64.23:  40%|████      | 12/30 [55:25<1:16:17, 254.31s/it]Task 2, Epoch 13/30 => Loss 1.488, Train_accy 66.64, Test_accy 64.23:  43%|████▎     | 13/30 [55:25<1:12:04, 254.36s/it]Task 2, Epoch 14/30 => Loss 1.502, Train_accy 66.34, Test_accy 63.88:  43%|████▎     | 13/30 [59:39<1:12:04, 254.36s/it]Task 2, Epoch 14/30 => Loss 1.502, Train_accy 66.34, Test_accy 63.88:  47%|████▋     | 14/30 [59:39<1:07:44, 254.03s/it]Task 2, Epoch 15/30 => Loss 1.487, Train_accy 66.51, Test_accy 63.83:  47%|████▋     | 14/30 [1:03:55<1:07:44, 254.03s/it]Task 2, Epoch 15/30 => Loss 1.487, Train_accy 66.51, Test_accy 63.83:  50%|█████     | 15/30 [1:03:55<1:03:40, 254.68s/it]Task 2, Epoch 16/30 => Loss 1.478, Train_accy 66.73, Test_accy 64.12:  50%|█████     | 15/30 [1:08:09<1:03:40, 254.68s/it]Task 2, Epoch 16/30 => Loss 1.478, Train_accy 66.73, Test_accy 64.12:  53%|█████▎    | 16/30 [1:08:09<59:22, 254.43s/it]  Task 2, Epoch 17/30 => Loss 1.487, Train_accy 66.76, Test_accy 63.61:  53%|█████▎    | 16/30 [1:12:22<59:22, 254.43s/it]Task 2, Epoch 17/30 => Loss 1.487, Train_accy 66.76, Test_accy 63.61:  57%|█████▋    | 17/30 [1:12:22<55:03, 254.11s/it]Task 2, Epoch 18/30 => Loss 1.482, Train_accy 66.59, Test_accy 63.76:  57%|█████▋    | 17/30 [1:16:37<55:03, 254.11s/it]Task 2, Epoch 18/30 => Loss 1.482, Train_accy 66.59, Test_accy 63.76:  60%|██████    | 18/30 [1:16:37<50:51, 254.30s/it]Task 2, Epoch 19/30 => Loss 1.466, Train_accy 67.24, Test_accy 63.90:  60%|██████    | 18/30 [1:20:52<50:51, 254.30s/it]Task 2, Epoch 19/30 => Loss 1.466, Train_accy 67.24, Test_accy 63.90:  63%|██████▎   | 19/30 [1:20:52<46:41, 254.69s/it]Task 2, Epoch 20/30 => Loss 1.460, Train_accy 66.93, Test_accy 63.91:  63%|██████▎   | 19/30 [1:25:05<46:41, 254.69s/it]Task 2, Epoch 20/30 => Loss 1.460, Train_accy 66.93, Test_accy 63.91:  67%|██████▋   | 20/30 [1:25:05<42:21, 254.16s/it]Task 2, Epoch 21/30 => Loss 1.463, Train_accy 67.11, Test_accy 63.62:  67%|██████▋   | 20/30 [1:29:22<42:21, 254.16s/it]Task 2, Epoch 21/30 => Loss 1.463, Train_accy 67.11, Test_accy 63.62:  70%|███████   | 21/30 [1:29:22<38:13, 254.84s/it]Task 2, Epoch 22/30 => Loss 1.459, Train_accy 67.19, Test_accy 63.94:  70%|███████   | 21/30 [1:33:37<38:13, 254.84s/it]Task 2, Epoch 22/30 => Loss 1.459, Train_accy 67.19, Test_accy 63.94:  73%|███████▎  | 22/30 [1:33:37<33:58, 254.87s/it]Task 2, Epoch 23/30 => Loss 1.455, Train_accy 67.22, Test_accy 63.67:  73%|███████▎  | 22/30 [1:37:52<33:58, 254.87s/it]Task 2, Epoch 23/30 => Loss 1.455, Train_accy 67.22, Test_accy 63.67:  77%|███████▋  | 23/30 [1:37:52<29:45, 255.03s/it]Task 2, Epoch 24/30 => Loss 1.452, Train_accy 67.26, Test_accy 63.63:  77%|███████▋  | 23/30 [1:42:06<29:45, 255.03s/it]Task 2, Epoch 24/30 => Loss 1.452, Train_accy 67.26, Test_accy 63.63:  80%|████████  | 24/30 [1:42:06<25:28, 254.82s/it]Task 2, Epoch 25/30 => Loss 1.439, Train_accy 67.36, Test_accy 63.97:  80%|████████  | 24/30 [1:46:20<25:28, 254.82s/it]Task 2, Epoch 25/30 => Loss 1.439, Train_accy 67.36, Test_accy 63.97:  83%|████████▎ | 25/30 [1:46:20<21:13, 254.60s/it]Task 2, Epoch 26/30 => Loss 1.451, Train_accy 67.10, Test_accy 63.53:  83%|████████▎ | 25/30 [1:50:35<21:13, 254.60s/it]Task 2, Epoch 26/30 => Loss 1.451, Train_accy 67.10, Test_accy 63.53:  87%|████████▋ | 26/30 [1:50:35<16:58, 254.61s/it]Task 2, Epoch 27/30 => Loss 1.443, Train_accy 67.47, Test_accy 63.57:  87%|████████▋ | 26/30 [1:54:47<16:58, 254.61s/it]Task 2, Epoch 27/30 => Loss 1.443, Train_accy 67.47, Test_accy 63.57:  90%|█████████ | 27/30 [1:54:47<12:41, 253.82s/it]Task 2, Epoch 28/30 => Loss 1.424, Train_accy 67.81, Test_accy 63.56:  90%|█████████ | 27/30 [1:59:01<12:41, 253.82s/it]Task 2, Epoch 28/30 => Loss 1.424, Train_accy 67.81, Test_accy 63.56:  93%|█████████▎| 28/30 [1:59:01<08:27, 253.96s/it]Task 2, Epoch 29/30 => Loss 1.441, Train_accy 67.48, Test_accy 63.54:  93%|█████████▎| 28/30 [2:03:14<08:27, 253.96s/it]Task 2, Epoch 29/30 => Loss 1.441, Train_accy 67.48, Test_accy 63.54:  97%|█████████▋| 29/30 [2:03:14<04:13, 253.67s/it]Task 2, Epoch 30/30 => Loss 1.440, Train_accy 67.45, Test_accy 63.54:  97%|█████████▋| 29/30 [2:07:28<04:13, 253.67s/it]Task 2, Epoch 30/30 => Loss 1.440, Train_accy 67.45, Test_accy 63.54: 100%|██████████| 30/30 [2:07:28<00:00, 253.83s/it]Task 2, Epoch 30/30 => Loss 1.440, Train_accy 67.45, Test_accy 63.54: 100%|██████████| 30/30 [2:07:28<00:00, 254.97s/it]
2024-06-14 14:44:46,697 [lora_prompt.py] => Task 2, Epoch 30/30 => Loss 1.440, Train_accy 67.45, Test_accy 63.54
2024-06-14 14:57:10,476 [lora_prompt.py] => Exemplar size: 0
2024-06-14 14:57:10,476 [trainer.py] => CNN: {'total': 70.44, '00-344': 77.18, '345-689': 57.42, '690-1034': 75.22, 'old': 66.98, 'new': 75.22}
2024-06-14 14:57:10,477 [trainer.py] => CNN top1 curve: [78.1, 67.47, 70.44]
2024-06-14 14:57:10,484 [trainer.py] => All params: 668051457
2024-06-14 14:57:10,490 [trainer.py] => Trainable params: 204800
2024-06-14 14:57:10,490 [lora_prompt.py] => Learning on 1035-1380
Parameters to be updated: {'lora_image_model_pool.3.base_model.model.transformer.resblocks.7.attn.out_proj.lora_A.weight', 'classifier_pool.3.ctx', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.7.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.9.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.10.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.1.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.5.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.4.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.11.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.8.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.2.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.11.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.3.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.0.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.9.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.6.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.0.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.4.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.10.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.6.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.1.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.3.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.5.attn.out_proj.lora_B.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.8.attn.out_proj.lora_A.weight', 'lora_image_model_pool.3.base_model.model.transformer.resblocks.2.attn.out_proj.lora_B.weight'},count:204800
  0%|          | 0/30 [00:00<?, ?it/s]Task 3, Epoch 1/30 => Loss 4.696, Train_accy 11.50, Test_accy 36.78:   0%|          | 0/30 [09:16<?, ?it/s]Task 3, Epoch 1/30 => Loss 4.696, Train_accy 11.50, Test_accy 36.78:   3%|▎         | 1/30 [09:16<4:28:57, 556.46s/it]Task 3, Epoch 2/30 => Loss 4.433, Train_accy 14.05, Test_accy 35.99:   3%|▎         | 1/30 [18:36<4:28:57, 556.46s/it]Task 3, Epoch 2/30 => Loss 4.433, Train_accy 14.05, Test_accy 35.99:   7%|▋         | 2/30 [18:36<4:20:40, 558.60s/it]Task 3, Epoch 3/30 => Loss 4.340, Train_accy 15.08, Test_accy 36.11:   7%|▋         | 2/30 [27:59<4:20:40, 558.60s/it]Task 3, Epoch 3/30 => Loss 4.340, Train_accy 15.08, Test_accy 36.11:  10%|█         | 3/30 [27:59<4:12:19, 560.73s/it]Task 3, Epoch 4/30 => Loss 4.284, Train_accy 15.64, Test_accy 35.39:  10%|█         | 3/30 [37:17<4:12:19, 560.73s/it]Task 3, Epoch 4/30 => Loss 4.284, Train_accy 15.64, Test_accy 35.39:  13%|█▎        | 4/30 [37:17<4:02:24, 559.40s/it]Task 3, Epoch 5/30 => Loss 4.231, Train_accy 16.19, Test_accy 35.83:  13%|█▎        | 4/30 [46:35<4:02:24, 559.40s/it]Task 3, Epoch 5/30 => Loss 4.231, Train_accy 16.19, Test_accy 35.83:  17%|█▋        | 5/30 [46:35<3:52:55, 559.02s/it]Task 3, Epoch 6/30 => Loss 4.196, Train_accy 16.69, Test_accy 35.52:  17%|█▋        | 5/30 [55:54<3:52:55, 559.02s/it]Task 3, Epoch 6/30 => Loss 4.196, Train_accy 16.69, Test_accy 35.52:  20%|██        | 6/30 [55:54<3:43:36, 559.00s/it]Task 3, Epoch 7/30 => Loss 4.167, Train_accy 16.99, Test_accy 34.89:  20%|██        | 6/30 [1:05:12<3:43:36, 559.00s/it]Task 3, Epoch 7/30 => Loss 4.167, Train_accy 16.99, Test_accy 34.89:  23%|██▎       | 7/30 [1:05:12<3:34:05, 558.52s/it]Task 3, Epoch 8/30 => Loss 4.145, Train_accy 17.33, Test_accy 35.87:  23%|██▎       | 7/30 [1:14:31<3:34:05, 558.52s/it]Task 3, Epoch 8/30 => Loss 4.145, Train_accy 17.33, Test_accy 35.87:  27%|██▋       | 8/30 [1:14:31<3:24:52, 558.75s/it]Task 3, Epoch 9/30 => Loss 4.131, Train_accy 17.38, Test_accy 34.62:  27%|██▋       | 8/30 [1:23:51<3:24:52, 558.75s/it]Task 3, Epoch 9/30 => Loss 4.131, Train_accy 17.38, Test_accy 34.62:  30%|███       | 9/30 [1:23:51<3:15:42, 559.18s/it]Task 3, Epoch 10/30 => Loss 4.110, Train_accy 17.68, Test_accy 35.28:  30%|███       | 9/30 [1:33:15<3:15:42, 559.18s/it]Task 3, Epoch 10/30 => Loss 4.110, Train_accy 17.68, Test_accy 35.28:  33%|███▎      | 10/30 [1:33:15<3:06:51, 560.58s/it]Task 3, Epoch 11/30 => Loss 4.097, Train_accy 17.96, Test_accy 35.09:  33%|███▎      | 10/30 [1:42:37<3:06:51, 560.58s/it]Task 3, Epoch 11/30 => Loss 4.097, Train_accy 17.96, Test_accy 35.09:  37%|███▋      | 11/30 [1:42:37<2:57:40, 561.06s/it]Task 3, Epoch 12/30 => Loss 4.078, Train_accy 18.39, Test_accy 35.25:  37%|███▋      | 11/30 [1:51:56<2:57:40, 561.06s/it]Task 3, Epoch 12/30 => Loss 4.078, Train_accy 18.39, Test_accy 35.25:  40%|████      | 12/30 [1:51:56<2:48:11, 560.61s/it]Task 3, Epoch 13/30 => Loss 4.068, Train_accy 18.50, Test_accy 35.20:  40%|████      | 12/30 [2:01:15<2:48:11, 560.61s/it]Task 3, Epoch 13/30 => Loss 4.068, Train_accy 18.50, Test_accy 35.20:  43%|████▎     | 13/30 [2:01:15<2:38:40, 560.02s/it]Task 3, Epoch 14/30 => Loss 4.052, Train_accy 18.63, Test_accy 35.26:  43%|████▎     | 13/30 [2:10:28<2:38:40, 560.02s/it]Task 3, Epoch 14/30 => Loss 4.052, Train_accy 18.63, Test_accy 35.26:  47%|████▋     | 14/30 [2:10:28<2:28:48, 558.05s/it]Task 3, Epoch 15/30 => Loss 4.036, Train_accy 18.67, Test_accy 35.24:  47%|████▋     | 14/30 [2:19:49<2:28:48, 558.05s/it]Task 3, Epoch 15/30 => Loss 4.036, Train_accy 18.67, Test_accy 35.24:  50%|█████     | 15/30 [2:19:49<2:19:42, 558.87s/it]Task 3, Epoch 16/30 => Loss 4.027, Train_accy 18.94, Test_accy 35.53:  50%|█████     | 15/30 [2:29:09<2:19:42, 558.87s/it]Task 3, Epoch 16/30 => Loss 4.027, Train_accy 18.94, Test_accy 35.53:  53%|█████▎    | 16/30 [2:29:09<2:10:27, 559.09s/it]Task 3, Epoch 17/30 => Loss 4.015, Train_accy 19.20, Test_accy 35.12:  53%|█████▎    | 16/30 [2:38:28<2:10:27, 559.09s/it]Task 3, Epoch 17/30 => Loss 4.015, Train_accy 19.20, Test_accy 35.12:  57%|█████▋    | 17/30 [2:38:28<2:01:09, 559.16s/it]Task 3, Epoch 18/30 => Loss 4.001, Train_accy 19.29, Test_accy 34.90:  57%|█████▋    | 17/30 [2:47:43<2:01:09, 559.16s/it]Task 3, Epoch 18/30 => Loss 4.001, Train_accy 19.29, Test_accy 34.90:  60%|██████    | 18/30 [2:47:43<1:51:32, 557.74s/it]Task 3, Epoch 19/30 => Loss 3.997, Train_accy 19.15, Test_accy 35.55:  60%|██████    | 18/30 [2:56:56<1:51:32, 557.74s/it]Task 3, Epoch 19/30 => Loss 3.997, Train_accy 19.15, Test_accy 35.55:  63%|██████▎   | 19/30 [2:56:56<1:41:59, 556.34s/it]Task 3, Epoch 20/30 => Loss 3.979, Train_accy 19.65, Test_accy 34.94:  63%|██████▎   | 19/30 [3:06:11<1:41:59, 556.34s/it]Task 3, Epoch 20/30 => Loss 3.979, Train_accy 19.65, Test_accy 34.94:  67%|██████▋   | 20/30 [3:06:11<1:32:41, 556.14s/it]Task 3, Epoch 21/30 => Loss 3.962, Train_accy 19.95, Test_accy 35.87:  67%|██████▋   | 20/30 [3:15:28<1:32:41, 556.14s/it]Task 3, Epoch 21/30 => Loss 3.962, Train_accy 19.95, Test_accy 35.87:  70%|███████   | 21/30 [3:15:28<1:23:27, 556.38s/it]Task 3, Epoch 22/30 => Loss 3.957, Train_accy 19.93, Test_accy 35.48:  70%|███████   | 21/30 [3:24:43<1:23:27, 556.38s/it]Task 3, Epoch 22/30 => Loss 3.957, Train_accy 19.93, Test_accy 35.48:  73%|███████▎  | 22/30 [3:24:43<1:14:07, 555.88s/it]Task 3, Epoch 23/30 => Loss 3.948, Train_accy 20.18, Test_accy 35.26:  73%|███████▎  | 22/30 [3:33:56<1:14:07, 555.88s/it]Task 3, Epoch 23/30 => Loss 3.948, Train_accy 20.18, Test_accy 35.26:  77%|███████▋  | 23/30 [3:33:56<1:04:45, 555.09s/it]Task 3, Epoch 24/30 => Loss 3.948, Train_accy 20.22, Test_accy 35.34:  77%|███████▋  | 23/30 [3:43:12<1:04:45, 555.09s/it]Task 3, Epoch 24/30 => Loss 3.948, Train_accy 20.22, Test_accy 35.34:  80%|████████  | 24/30 [3:43:12<55:32, 555.37s/it]  Task 3, Epoch 25/30 => Loss 3.929, Train_accy 20.35, Test_accy 35.38:  80%|████████  | 24/30 [3:52:21<55:32, 555.37s/it]Task 3, Epoch 25/30 => Loss 3.929, Train_accy 20.35, Test_accy 35.38:  83%|████████▎ | 25/30 [3:52:21<46:07, 553.45s/it]Task 3, Epoch 26/30 => Loss 3.926, Train_accy 20.47, Test_accy 35.31:  83%|████████▎ | 25/30 [4:01:33<46:07, 553.45s/it]Task 3, Epoch 26/30 => Loss 3.926, Train_accy 20.47, Test_accy 35.31:  87%|████████▋ | 26/30 [4:01:33<36:52, 553.04s/it]Task 3, Epoch 27/30 => Loss 3.922, Train_accy 20.45, Test_accy 35.41:  87%|████████▋ | 26/30 [4:10:45<36:52, 553.04s/it]Task 3, Epoch 27/30 => Loss 3.922, Train_accy 20.45, Test_accy 35.41:  90%|█████████ | 27/30 [4:10:45<27:37, 552.57s/it]Task 3, Epoch 28/30 => Loss 3.907, Train_accy 20.78, Test_accy 35.68:  90%|█████████ | 27/30 [4:19:58<27:37, 552.57s/it]Task 3, Epoch 28/30 => Loss 3.907, Train_accy 20.78, Test_accy 35.68:  93%|█████████▎| 28/30 [4:19:58<18:25, 552.87s/it]Task 3, Epoch 29/30 => Loss 3.913, Train_accy 20.61, Test_accy 35.72:  93%|█████████▎| 28/30 [4:29:11<18:25, 552.87s/it]Task 3, Epoch 29/30 => Loss 3.913, Train_accy 20.61, Test_accy 35.72:  97%|█████████▋| 29/30 [4:29:11<09:12, 552.83s/it]Task 3, Epoch 30/30 => Loss 3.902, Train_accy 20.91, Test_accy 35.73:  97%|█████████▋| 29/30 [4:38:22<09:12, 552.83s/it]Task 3, Epoch 30/30 => Loss 3.902, Train_accy 20.91, Test_accy 35.73: 100%|██████████| 30/30 [4:38:22<00:00, 552.33s/it]Task 3, Epoch 30/30 => Loss 3.902, Train_accy 20.91, Test_accy 35.73: 100%|██████████| 30/30 [4:38:22<00:00, 556.76s/it]
2024-06-14 19:35:34,112 [lora_prompt.py] => Task 3, Epoch 30/30 => Loss 3.902, Train_accy 20.91, Test_accy 35.73
2024-06-14 20:00:11,742 [lora_prompt.py] => Exemplar size: 0
2024-06-14 20:00:11,742 [trainer.py] => CNN: {'total': 53.82, '00-344': 77.05, '345-689': 57.41, '690-1034': 75.18, '1035-1379': 37.16, 'old': 70.38, 'new': 37.16}
2024-06-14 20:00:11,742 [trainer.py] => CNN top1 curve: [78.1, 67.47, 70.44, 53.82]
2024-06-14 20:00:11,748 [trainer.py] => All params: 668051457
2024-06-14 20:00:11,753 [trainer.py] => Trainable params: 204800
2024-06-14 20:00:11,754 [lora_prompt.py] => Learning on 1380-1725
Parameters to be updated: {'lora_image_model_pool.4.base_model.model.transformer.resblocks.1.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.5.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.11.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.10.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.5.attn.out_proj.lora_A.weight', 'classifier_pool.4.ctx', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.1.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.8.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.4.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.9.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.4.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.7.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.11.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.3.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.3.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.2.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.10.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.6.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.2.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.8.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.6.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.9.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.0.attn.out_proj.lora_B.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.0.attn.out_proj.lora_A.weight', 'lora_image_model_pool.4.base_model.model.transformer.resblocks.7.attn.out_proj.lora_A.weight'},count:204800
  0%|          | 0/30 [00:00<?, ?it/s]Task 4, Epoch 1/30 => Loss 0.970, Train_accy 76.98, Test_accy 54.35:   0%|          | 0/30 [10:53<?, ?it/s]Task 4, Epoch 1/30 => Loss 0.970, Train_accy 76.98, Test_accy 54.35:   3%|▎         | 1/30 [10:53<5:16:00, 653.80s/it]Task 4, Epoch 2/30 => Loss 0.921, Train_accy 77.80, Test_accy 54.18:   3%|▎         | 1/30 [21:48<5:16:00, 653.80s/it]Task 4, Epoch 2/30 => Loss 0.921, Train_accy 77.80, Test_accy 54.18:   7%|▋         | 2/30 [21:48<5:05:26, 654.53s/it]Task 4, Epoch 3/30 => Loss 0.907, Train_accy 78.09, Test_accy 54.10:   7%|▋         | 2/30 [32:39<5:05:26, 654.53s/it]Task 4, Epoch 3/30 => Loss 0.907, Train_accy 78.09, Test_accy 54.10:  10%|█         | 3/30 [32:39<4:53:45, 652.78s/it]Task 4, Epoch 4/30 => Loss 0.898, Train_accy 78.31, Test_accy 54.19:  10%|█         | 3/30 [43:26<4:53:45, 652.78s/it]Task 4, Epoch 4/30 => Loss 0.898, Train_accy 78.31, Test_accy 54.19:  13%|█▎        | 4/30 [43:26<4:41:55, 650.61s/it]Task 4, Epoch 5/30 => Loss 0.892, Train_accy 78.19, Test_accy 54.10:  13%|█▎        | 4/30 [54:18<4:41:55, 650.61s/it]Task 4, Epoch 5/30 => Loss 0.892, Train_accy 78.19, Test_accy 54.10:  17%|█▋        | 5/30 [54:18<4:31:15, 651.01s/it]Task 4, Epoch 6/30 => Loss 0.882, Train_accy 78.56, Test_accy 54.08:  17%|█▋        | 5/30 [1:05:15<4:31:15, 651.01s/it]Task 4, Epoch 6/30 => Loss 0.882, Train_accy 78.56, Test_accy 54.08:  20%|██        | 6/30 [1:05:15<4:21:13, 653.07s/it]Task 4, Epoch 7/30 => Loss 0.881, Train_accy 78.58, Test_accy 54.10:  20%|██        | 6/30 [1:16:02<4:21:13, 653.07s/it]Task 4, Epoch 7/30 => Loss 0.881, Train_accy 78.58, Test_accy 54.10:  23%|██▎       | 7/30 [1:16:02<4:09:36, 651.16s/it]Task 4, Epoch 8/30 => Loss 0.873, Train_accy 78.74, Test_accy 54.14:  23%|██▎       | 7/30 [1:26:53<4:09:36, 651.16s/it]Task 4, Epoch 8/30 => Loss 0.873, Train_accy 78.74, Test_accy 54.14:  27%|██▋       | 8/30 [1:26:53<3:58:39, 650.90s/it]Task 4, Epoch 9/30 => Loss 0.871, Train_accy 78.74, Test_accy 54.07:  27%|██▋       | 8/30 [1:37:42<3:58:39, 650.90s/it]Task 4, Epoch 9/30 => Loss 0.871, Train_accy 78.74, Test_accy 54.07:  30%|███       | 9/30 [1:37:42<3:47:35, 650.28s/it]Task 4, Epoch 10/30 => Loss 0.874, Train_accy 78.55, Test_accy 54.11:  30%|███       | 9/30 [1:48:30<3:47:35, 650.28s/it]Task 4, Epoch 10/30 => Loss 0.874, Train_accy 78.55, Test_accy 54.11:  33%|███▎      | 10/30 [1:48:30<3:36:31, 649.56s/it]Task 4, Epoch 11/30 => Loss 0.869, Train_accy 78.78, Test_accy 53.83:  33%|███▎      | 10/30 [1:59:16<3:36:31, 649.56s/it]Task 4, Epoch 11/30 => Loss 0.869, Train_accy 78.78, Test_accy 53.83:  37%|███▋      | 11/30 [1:59:16<3:25:21, 648.49s/it]Task 4, Epoch 12/30 => Loss 0.867, Train_accy 78.82, Test_accy 54.11:  37%|███▋      | 11/30 [2:10:03<3:25:21, 648.49s/it]Task 4, Epoch 12/30 => Loss 0.867, Train_accy 78.82, Test_accy 54.11:  40%|████      | 12/30 [2:10:03<3:14:29, 648.30s/it]Task 4, Epoch 13/30 => Loss 0.863, Train_accy 78.90, Test_accy 53.94:  40%|████      | 12/30 [2:20:48<3:14:29, 648.30s/it]Task 4, Epoch 13/30 => Loss 0.863, Train_accy 78.90, Test_accy 53.94:  43%|████▎     | 13/30 [2:20:48<3:03:21, 647.17s/it]Task 4, Epoch 14/30 => Loss 0.863, Train_accy 79.05, Test_accy 53.99:  43%|████▎     | 13/30 [2:31:31<3:03:21, 647.17s/it]Task 4, Epoch 14/30 => Loss 0.863, Train_accy 79.05, Test_accy 53.99:  47%|████▋     | 14/30 [2:31:31<2:52:16, 646.04s/it]Task 4, Epoch 15/30 => Loss 0.858, Train_accy 78.98, Test_accy 53.50:  47%|████▋     | 14/30 [2:42:18<2:52:16, 646.04s/it]Task 4, Epoch 15/30 => Loss 0.858, Train_accy 78.98, Test_accy 53.50:  50%|█████     | 15/30 [2:42:18<2:41:34, 646.27s/it]Task 4, Epoch 16/30 => Loss 0.859, Train_accy 79.11, Test_accy 54.14:  50%|█████     | 15/30 [2:53:05<2:41:34, 646.27s/it]Task 4, Epoch 16/30 => Loss 0.859, Train_accy 79.11, Test_accy 54.14:  53%|█████▎    | 16/30 [2:53:05<2:30:50, 646.45s/it]Task 4, Epoch 17/30 => Loss 0.857, Train_accy 78.96, Test_accy 53.84:  53%|█████▎    | 16/30 [3:03:50<2:30:50, 646.45s/it]Task 4, Epoch 17/30 => Loss 0.857, Train_accy 78.96, Test_accy 53.84:  57%|█████▋    | 17/30 [3:03:50<2:19:58, 646.05s/it]Task 4, Epoch 18/30 => Loss 0.854, Train_accy 79.00, Test_accy 53.94:  57%|█████▋    | 17/30 [3:14:36<2:19:58, 646.05s/it]Task 4, Epoch 18/30 => Loss 0.854, Train_accy 79.00, Test_accy 53.94:  60%|██████    | 18/30 [3:14:36<2:09:12, 646.02s/it]Task 4, Epoch 19/30 => Loss 0.854, Train_accy 79.11, Test_accy 53.83:  60%|██████    | 18/30 [3:25:26<2:09:12, 646.02s/it]Task 4, Epoch 19/30 => Loss 0.854, Train_accy 79.11, Test_accy 53.83:  63%|██████▎   | 19/30 [3:25:26<1:58:38, 647.13s/it]Task 4, Epoch 20/30 => Loss 0.848, Train_accy 79.14, Test_accy 54.05:  63%|██████▎   | 19/30 [3:36:15<1:58:38, 647.13s/it]Task 4, Epoch 20/30 => Loss 0.848, Train_accy 79.14, Test_accy 54.05:  67%|██████▋   | 20/30 [3:36:15<1:47:56, 647.65s/it]Task 4, Epoch 21/30 => Loss 0.851, Train_accy 79.11, Test_accy 53.89:  67%|██████▋   | 20/30 [3:47:02<1:47:56, 647.65s/it]Task 4, Epoch 21/30 => Loss 0.851, Train_accy 79.11, Test_accy 53.89:  70%|███████   | 21/30 [3:47:02<1:37:08, 647.58s/it]Task 4, Epoch 22/30 => Loss 0.844, Train_accy 79.24, Test_accy 53.98:  70%|███████   | 21/30 [3:57:46<1:37:08, 647.58s/it]Task 4, Epoch 22/30 => Loss 0.844, Train_accy 79.24, Test_accy 53.98:  73%|███████▎  | 22/30 [3:57:46<1:26:11, 646.43s/it]Task 4, Epoch 23/30 => Loss 0.840, Train_accy 79.34, Test_accy 53.86:  73%|███████▎  | 22/30 [4:08:34<1:26:11, 646.43s/it]Task 4, Epoch 23/30 => Loss 0.840, Train_accy 79.34, Test_accy 53.86:  77%|███████▋  | 23/30 [4:08:34<1:15:27, 646.79s/it]Task 4, Epoch 24/30 => Loss 0.845, Train_accy 79.21, Test_accy 53.89:  77%|███████▋  | 23/30 [4:19:20<1:15:27, 646.79s/it]Task 4, Epoch 24/30 => Loss 0.845, Train_accy 79.21, Test_accy 53.89:  80%|████████  | 24/30 [4:19:20<1:04:40, 646.69s/it]Task 4, Epoch 25/30 => Loss 0.845, Train_accy 79.12, Test_accy 53.77:  80%|████████  | 24/30 [4:30:08<1:04:40, 646.69s/it]Task 4, Epoch 25/30 => Loss 0.845, Train_accy 79.12, Test_accy 53.77:  83%|████████▎ | 25/30 [4:30:08<53:55, 647.14s/it]  Task 4, Epoch 26/30 => Loss 0.834, Train_accy 79.44, Test_accy 53.84:  83%|████████▎ | 25/30 [4:40:57<53:55, 647.14s/it]Task 4, Epoch 26/30 => Loss 0.834, Train_accy 79.44, Test_accy 53.84:  87%|████████▋ | 26/30 [4:40:57<43:10, 647.67s/it]Task 4, Epoch 27/30 => Loss 0.835, Train_accy 79.27, Test_accy 53.86:  87%|████████▋ | 26/30 [4:51:42<43:10, 647.67s/it]Task 4, Epoch 27/30 => Loss 0.835, Train_accy 79.27, Test_accy 53.86:  90%|█████████ | 27/30 [4:51:42<32:20, 646.83s/it]Task 4, Epoch 28/30 => Loss 0.841, Train_accy 79.25, Test_accy 53.84:  90%|█████████ | 27/30 [5:02:28<32:20, 646.83s/it]Task 4, Epoch 28/30 => Loss 0.841, Train_accy 79.25, Test_accy 53.84:  93%|█████████▎| 28/30 [5:02:28<21:33, 646.69s/it]Task 4, Epoch 29/30 => Loss 0.839, Train_accy 79.41, Test_accy 53.82:  93%|█████████▎| 28/30 [5:13:16<21:33, 646.69s/it]Task 4, Epoch 29/30 => Loss 0.839, Train_accy 79.41, Test_accy 53.82:  97%|█████████▋| 29/30 [5:13:16<10:46, 646.98s/it]Task 4, Epoch 30/30 => Loss 0.841, Train_accy 79.47, Test_accy 53.82:  97%|█████████▋| 29/30 [5:24:05<10:46, 646.98s/it]Task 4, Epoch 30/30 => Loss 0.841, Train_accy 79.47, Test_accy 53.82: 100%|██████████| 30/30 [5:24:05<00:00, 647.64s/it]Task 4, Epoch 30/30 => Loss 0.841, Train_accy 79.47, Test_accy 53.82: 100%|██████████| 30/30 [5:24:05<00:00, 648.19s/it]
2024-06-15 01:24:18,395 [lora_prompt.py] => Task 4, Epoch 30/30 => Loss 0.841, Train_accy 79.47, Test_accy 53.82
2024-06-15 02:00:15,118 [lora_prompt.py] => Exemplar size: 0
2024-06-15 02:00:15,119 [trainer.py] => CNN: {'total': 63.98, '00-344': 76.99, '345-689': 57.53, '690-1034': 74.14, '1035-1379': 37.16, '1380-1724': 84.66, 'old': 53.61, 'new': 84.66}
2024-06-15 02:00:15,119 [trainer.py] => CNN top1 curve: [78.1, 67.47, 70.44, 53.82, 63.98]
2024-06-15 02:00:15,126 [trainer.py] => All params: 668051457
2024-06-15 02:00:15,131 [trainer.py] => Trainable params: 204800
2024-06-15 02:00:15,131 [lora_prompt.py] => Learning on 1725-2070
Parameters to be updated: {'lora_image_model_pool.5.base_model.model.transformer.resblocks.7.attn.out_proj.lora_B.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.9.attn.out_proj.lora_A.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.1.attn.out_proj.lora_A.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.4.attn.out_proj.lora_B.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.1.attn.out_proj.lora_B.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.0.attn.out_proj.lora_A.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.4.attn.out_proj.lora_A.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.10.attn.out_proj.lora_B.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.5.attn.out_proj.lora_B.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.10.attn.out_proj.lora_A.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.11.attn.out_proj.lora_A.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.3.attn.out_proj.lora_B.weight', 'classifier_pool.5.ctx', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.3.attn.out_proj.lora_A.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.9.attn.out_proj.lora_B.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.0.attn.out_proj.lora_B.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.2.attn.out_proj.lora_A.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.8.attn.out_proj.lora_A.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.7.attn.out_proj.lora_A.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.11.attn.out_proj.lora_B.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.6.attn.out_proj.lora_A.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.6.attn.out_proj.lora_B.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.5.attn.out_proj.lora_A.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.8.attn.out_proj.lora_B.weight', 'lora_image_model_pool.5.base_model.model.transformer.resblocks.2.attn.out_proj.lora_B.weight'},count:204800
  0%|          | 0/30 [00:00<?, ?it/s]Task 5, Epoch 1/30 => Loss 2.213, Train_accy 54.09, Test_accy 54.78:   0%|          | 0/30 [07:57<?, ?it/s]Task 5, Epoch 1/30 => Loss 2.213, Train_accy 54.09, Test_accy 54.78:   3%|▎         | 1/30 [07:57<3:50:36, 477.13s/it]Task 5, Epoch 2/30 => Loss 2.085, Train_accy 55.50, Test_accy 52.84:   3%|▎         | 1/30 [15:53<3:50:36, 477.13s/it]Task 5, Epoch 2/30 => Loss 2.085, Train_accy 55.50, Test_accy 52.84:   7%|▋         | 2/30 [15:53<3:42:34, 476.93s/it]Task 5, Epoch 3/30 => Loss 2.032, Train_accy 56.48, Test_accy 53.87:   7%|▋         | 2/30 [23:55<3:42:34, 476.93s/it]Task 5, Epoch 3/30 => Loss 2.032, Train_accy 56.48, Test_accy 53.87:  10%|█         | 3/30 [23:55<3:35:28, 478.83s/it]Task 5, Epoch 4/30 => Loss 2.010, Train_accy 56.70, Test_accy 53.41:  10%|█         | 3/30 [31:52<3:35:28, 478.83s/it]Task 5, Epoch 4/30 => Loss 2.010, Train_accy 56.70, Test_accy 53.41:  13%|█▎        | 4/30 [31:52<3:27:14, 478.25s/it]Task 5, Epoch 5/30 => Loss 1.992, Train_accy 56.84, Test_accy 52.19:  13%|█▎        | 4/30 [39:48<3:27:14, 478.25s/it]Task 5, Epoch 5/30 => Loss 1.992, Train_accy 56.84, Test_accy 52.19:  17%|█▋        | 5/30 [39:48<3:18:56, 477.46s/it]Task 5, Epoch 6/30 => Loss 1.979, Train_accy 57.11, Test_accy 52.56:  17%|█▋        | 5/30 [47:44<3:18:56, 477.46s/it]Task 5, Epoch 6/30 => Loss 1.979, Train_accy 57.11, Test_accy 52.56:  20%|██        | 6/30 [47:44<3:10:51, 477.13s/it]Task 5, Epoch 7/30 => Loss 1.962, Train_accy 57.36, Test_accy 53.16:  20%|██        | 6/30 [55:40<3:10:51, 477.13s/it]Task 5, Epoch 7/30 => Loss 1.962, Train_accy 57.36, Test_accy 53.16:  23%|██▎       | 7/30 [55:40<3:02:43, 476.68s/it]Task 5, Epoch 8/30 => Loss 1.962, Train_accy 57.13, Test_accy 50.48:  23%|██▎       | 7/30 [1:03:36<3:02:43, 476.68s/it]Task 5, Epoch 8/30 => Loss 1.962, Train_accy 57.13, Test_accy 50.48:  27%|██▋       | 8/30 [1:03:36<2:54:43, 476.52s/it]Task 5, Epoch 9/30 => Loss 1.942, Train_accy 57.79, Test_accy 50.80:  27%|██▋       | 8/30 [1:11:33<2:54:43, 476.52s/it]Task 5, Epoch 9/30 => Loss 1.942, Train_accy 57.79, Test_accy 50.80:  30%|███       | 9/30 [1:11:33<2:46:46, 476.50s/it]Task 5, Epoch 10/30 => Loss 1.939, Train_accy 57.64, Test_accy 51.29:  30%|███       | 9/30 [1:19:27<2:46:46, 476.50s/it]Task 5, Epoch 10/30 => Loss 1.939, Train_accy 57.64, Test_accy 51.29:  33%|███▎      | 10/30 [1:19:27<2:38:37, 475.88s/it]Task 5, Epoch 11/30 => Loss 1.945, Train_accy 57.78, Test_accy 50.60:  33%|███▎      | 10/30 [1:27:23<2:38:37, 475.88s/it]Task 5, Epoch 11/30 => Loss 1.945, Train_accy 57.78, Test_accy 50.60:  37%|███▋      | 11/30 [1:27:23<2:30:40, 475.81s/it]Task 5, Epoch 12/30 => Loss 1.927, Train_accy 57.79, Test_accy 50.94:  37%|███▋      | 11/30 [1:35:19<2:30:40, 475.81s/it]Task 5, Epoch 12/30 => Loss 1.927, Train_accy 57.79, Test_accy 50.94:  40%|████      | 12/30 [1:35:19<2:22:44, 475.78s/it]Task 5, Epoch 13/30 => Loss 1.914, Train_accy 58.11, Test_accy 50.89:  40%|████      | 12/30 [1:43:12<2:22:44, 475.78s/it]Task 5, Epoch 13/30 => Loss 1.914, Train_accy 58.11, Test_accy 50.89:  43%|████▎     | 13/30 [1:43:12<2:14:33, 474.93s/it]Task 5, Epoch 14/30 => Loss 1.903, Train_accy 58.20, Test_accy 51.41:  43%|████▎     | 13/30 [1:51:05<2:14:33, 474.93s/it]Task 5, Epoch 14/30 => Loss 1.903, Train_accy 58.20, Test_accy 51.41:  47%|████▋     | 14/30 [1:51:05<2:06:31, 474.49s/it]Task 5, Epoch 15/30 => Loss 1.909, Train_accy 58.24, Test_accy 50.58:  47%|████▋     | 14/30 [1:59:02<2:06:31, 474.49s/it]Task 5, Epoch 15/30 => Loss 1.909, Train_accy 58.24, Test_accy 50.58:  50%|█████     | 15/30 [1:59:02<1:58:49, 475.29s/it]Task 5, Epoch 16/30 => Loss 1.906, Train_accy 58.36, Test_accy 50.63:  50%|█████     | 15/30 [2:06:58<1:58:49, 475.29s/it]Task 5, Epoch 16/30 => Loss 1.906, Train_accy 58.36, Test_accy 50.63:  53%|█████▎    | 16/30 [2:06:58<1:50:56, 475.44s/it]Task 5, Epoch 17/30 => Loss 1.898, Train_accy 58.38, Test_accy 52.10:  53%|█████▎    | 16/30 [2:14:51<1:50:56, 475.44s/it]Task 5, Epoch 17/30 => Loss 1.898, Train_accy 58.38, Test_accy 52.10:  57%|█████▋    | 17/30 [2:14:51<1:42:51, 474.71s/it]Task 5, Epoch 18/30 => Loss 1.899, Train_accy 58.22, Test_accy 50.99:  57%|█████▋    | 17/30 [2:22:46<1:42:51, 474.71s/it]Task 5, Epoch 18/30 => Loss 1.899, Train_accy 58.22, Test_accy 50.99:  60%|██████    | 18/30 [2:22:46<1:34:55, 474.65s/it]Task 5, Epoch 19/30 => Loss 1.898, Train_accy 58.37, Test_accy 51.36:  60%|██████    | 18/30 [2:30:42<1:34:55, 474.65s/it]Task 5, Epoch 19/30 => Loss 1.898, Train_accy 58.37, Test_accy 51.36:  63%|██████▎   | 19/30 [2:30:42<1:27:05, 475.07s/it]Task 5, Epoch 20/30 => Loss 1.890, Train_accy 58.60, Test_accy 51.07:  63%|██████▎   | 19/30 [2:38:32<1:27:05, 475.07s/it]Task 5, Epoch 20/30 => Loss 1.890, Train_accy 58.60, Test_accy 51.07:  67%|██████▋   | 20/30 [2:38:32<1:18:56, 473.69s/it]Task 5, Epoch 21/30 => Loss 1.904, Train_accy 58.40, Test_accy 50.64:  67%|██████▋   | 20/30 [2:46:26<1:18:56, 473.69s/it]Task 5, Epoch 21/30 => Loss 1.904, Train_accy 58.40, Test_accy 50.64:  70%|███████   | 21/30 [2:46:26<1:11:02, 473.66s/it]Task 5, Epoch 22/30 => Loss 1.874, Train_accy 58.67, Test_accy 50.97:  70%|███████   | 21/30 [2:54:20<1:11:02, 473.66s/it]Task 5, Epoch 22/30 => Loss 1.874, Train_accy 58.67, Test_accy 50.97:  73%|███████▎  | 22/30 [2:54:20<1:03:11, 473.97s/it]Task 5, Epoch 23/30 => Loss 1.871, Train_accy 58.89, Test_accy 50.64:  73%|███████▎  | 22/30 [3:02:17<1:03:11, 473.97s/it]Task 5, Epoch 23/30 => Loss 1.871, Train_accy 58.89, Test_accy 50.64:  77%|███████▋  | 23/30 [3:02:17<55:23, 474.78s/it]  Task 5, Epoch 24/30 => Loss 1.874, Train_accy 58.80, Test_accy 50.43:  77%|███████▋  | 23/30 [3:10:13<55:23, 474.78s/it]Task 5, Epoch 24/30 => Loss 1.874, Train_accy 58.80, Test_accy 50.43:  80%|████████  | 24/30 [3:10:13<47:31, 475.20s/it]Task 5, Epoch 25/30 => Loss 1.867, Train_accy 59.09, Test_accy 49.96:  80%|████████  | 24/30 [3:18:11<47:31, 475.20s/it]Task 5, Epoch 25/30 => Loss 1.867, Train_accy 59.09, Test_accy 49.96:  83%|████████▎ | 25/30 [3:18:11<39:39, 475.95s/it]Task 5, Epoch 26/30 => Loss 1.855, Train_accy 59.10, Test_accy 50.20:  83%|████████▎ | 25/30 [3:26:07<39:39, 475.95s/it]Task 5, Epoch 26/30 => Loss 1.855, Train_accy 59.10, Test_accy 50.20:  87%|████████▋ | 26/30 [3:26:07<31:44, 476.07s/it]Task 5, Epoch 27/30 => Loss 1.865, Train_accy 58.96, Test_accy 50.13:  87%|████████▋ | 26/30 [3:34:06<31:44, 476.07s/it]Task 5, Epoch 27/30 => Loss 1.865, Train_accy 58.96, Test_accy 50.13:  90%|█████████ | 27/30 [3:34:06<23:50, 476.84s/it]Task 5, Epoch 28/30 => Loss 1.849, Train_accy 59.29, Test_accy 50.09:  90%|█████████ | 27/30 [3:42:02<23:50, 476.84s/it]Task 5, Epoch 28/30 => Loss 1.849, Train_accy 59.29, Test_accy 50.09:  93%|█████████▎| 28/30 [3:42:02<15:53, 476.55s/it]Task 5, Epoch 29/30 => Loss 1.863, Train_accy 59.01, Test_accy 50.15:  93%|█████████▎| 28/30 [3:49:57<15:53, 476.55s/it]Task 5, Epoch 29/30 => Loss 1.863, Train_accy 59.01, Test_accy 50.15:  97%|█████████▋| 29/30 [3:49:57<07:56, 476.07s/it]Task 5, Epoch 30/30 => Loss 1.863, Train_accy 58.84, Test_accy 50.13:  97%|█████████▋| 29/30 [3:57:50<07:56, 476.07s/it]Task 5, Epoch 30/30 => Loss 1.863, Train_accy 58.84, Test_accy 50.13: 100%|██████████| 30/30 [3:57:50<00:00, 475.33s/it]Task 5, Epoch 30/30 => Loss 1.863, Train_accy 58.84, Test_accy 50.13: 100%|██████████| 30/30 [3:57:50<00:00, 475.69s/it]
2024-06-15 05:58:06,828 [lora_prompt.py] => Task 5, Epoch 30/30 => Loss 1.863, Train_accy 58.84, Test_accy 50.13
2024-06-15 06:37:24,669 [lora_prompt.py] => Exemplar size: 0
2024-06-15 06:37:24,669 [trainer.py] => CNN: {'total': 64.45, '00-344': 76.21, '345-689': 57.52, '690-1034': 73.84, '1035-1379': 37.15, '1380-1724': 84.64, '1725-2069': 68.9, 'old': 63.85, 'new': 68.9}
2024-06-15 06:37:24,669 [trainer.py] => CNN top1 curve: [78.1, 67.47, 70.44, 53.82, 63.98, 64.45]
